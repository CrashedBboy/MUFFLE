{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cacsade Network + User Profile + User Timeline + Source Tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import warnings\n",
    "import os.path as path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Transformer / BERT\n",
    "import transformers as ppb\n",
    "from transformers import AdamW\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Torch-Geometric\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# print messages\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device type: {device.type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NEWS = \"./data/all/news.csv\"\n",
    "\n",
    "news_df = pd.read_csv(DATASET_NEWS, header = 0)\n",
    "print(f\"Columns: {news_df.columns}\")\n",
    "print(f\"news set size: {len(news_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ST_DATASET_TRAIN = \"./data/splitted/source_tweet_train.csv\"\n",
    "ST_DATASET_CV = \"./data/splitted/source_tweet_cv.csv\"\n",
    "ST_DATASET_TEST = \"./data/splitted/source_tweet_test.csv\"\n",
    "\n",
    "st_train_df = pd.read_csv(ST_DATASET_TRAIN, header = 0)\n",
    "st_cv_df = pd.read_csv(ST_DATASET_CV, header = 0)\n",
    "st_test_df = pd.read_csv(ST_DATASET_TEST, header = 0)\n",
    "\n",
    "print(f\"Columns: {st_train_df.columns}\")\n",
    "print(f\"train set size: {len(st_train_df)}\")\n",
    "print(f\"cross validation set size: {len(st_cv_df)}\")\n",
    "print(f\"test set size: {len(st_test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UP_DATASET_TRAIN = \"./data/splitted/user_profile_processed_train.csv\"\n",
    "UP_DATASET_CV = \"./data/splitted/user_profile_processed_cv.csv\"\n",
    "UP_DATASET_TEST = \"./data/splitted/user_profile_processed_test.csv\"\n",
    "\n",
    "up_train_df = pd.read_csv(UP_DATASET_TRAIN, header = 0)\n",
    "up_cv_df = pd.read_csv(UP_DATASET_CV, header = 0)\n",
    "up_test_df = pd.read_csv(UP_DATASET_TEST, header = 0)\n",
    "\n",
    "print(f\"Columns: {up_train_df.columns}\")\n",
    "print(f\"train set size: {len(up_train_df)}\")\n",
    "print(f\"cross validation set size: {len(up_cv_df)}\")\n",
    "print(f\"test set size: {len(up_test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UT_DATASET_TRAIN = \"./data/splitted/user_timeline_processed_train.csv\"\n",
    "UT_DATASET_CV = \"./data/splitted/user_timeline_processed_cv.csv\"\n",
    "UT_DATASET_TEST = \"./data/splitted/user_timeline_processed_test.csv\"\n",
    "\n",
    "ut_train_df = pd.read_csv(UT_DATASET_TRAIN, header = 0)\n",
    "ut_cv_df = pd.read_csv(UT_DATASET_CV, header = 0)\n",
    "ut_test_df = pd.read_csv(UT_DATASET_TEST, header = 0)\n",
    "\n",
    "print(f\"Columns: {ut_train_df.columns}\")\n",
    "print(f\"train set size: {len(ut_train_df)}\")\n",
    "print(f\"cross validation set size: {len(ut_cv_df)}\")\n",
    "print(f\"test set size: {len(ut_test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CN_DATASET_TRAIN = \"./data/splitted/cascade_network_train.csv\"\n",
    "CN_DATASET_CV = \"./data/splitted/cascade_network_cv.csv\"\n",
    "CN_DATASET_TEST = \"./data/splitted/cascade_network_test.csv\"\n",
    "\n",
    "cn_train_df = pd.read_csv(CN_DATASET_TRAIN, header = 0)\n",
    "cn_cv_df = pd.read_csv(CN_DATASET_CV, header = 0)\n",
    "cn_test_df = pd.read_csv(CN_DATASET_TEST, header = 0)\n",
    "\n",
    "print(f\"Columns: {cn_train_df.columns}\")\n",
    "print(f\"train set size: {len(cn_train_df)}\")\n",
    "print(f\"cross validation set size: {len(cn_cv_df)}\")\n",
    "print(f\"test set size: {len(cn_test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained tokenizer\n",
    "\n",
    "# For DistilBERT:\n",
    "tokenizer = ppb.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_title_tokenized = news_df['title'].apply(lambda x: tokenizer.encode(x, add_special_tokens = True))\n",
    "print(f\"news title tokenized shape: {news_title_tokenized.shape}\")\n",
    "\n",
    "news_text_tokenized = news_df['text'].apply(lambda x: tokenizer.encode(x, add_special_tokens = True))\n",
    "print(f\"news test tokenized shape: {news_text_tokenized.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWS_TITLE_LENGTH = 20\n",
    "NEWS_TEXT_LENGTH = 64\n",
    "\n",
    "# truncate news title and text \n",
    "for i in range(len(news_title_tokenized)):\n",
    "    news_title_tokenized[i] = news_title_tokenized[i][:NEWS_TITLE_LENGTH]\n",
    "for i in range(len(news_text_tokenized)):\n",
    "    news_text_tokenized[i] = news_text_tokenized[i][:NEWS_TEXT_LENGTH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding\n",
    "news_title_padded = np.array([i + [0] * (NEWS_TITLE_LENGTH - len(i)) for i in news_title_tokenized.values])\n",
    "news_text_padded = np.array([i + [0] * (NEWS_TEXT_LENGTH - len(i)) for i in news_text_tokenized.values])\n",
    "\n",
    "print(f\"news_title_padded: {news_title_padded.shape}\")\n",
    "print(f\"news_text_padded: {news_text_padded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention mask\n",
    "news_title_attention_mask = np.where(news_title_padded != 0, 1, 0)\n",
    "news_text_attention_mask = np.where(news_text_padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWS_NUMERIC_COLUMNS = ['img_count']\n",
    "\n",
    "# news text\n",
    "train_news_text_tensors = []\n",
    "cv_news_text_tensors = []\n",
    "test_news_text_tensors = []\n",
    "train_news_text_mask = []\n",
    "cv_news_text_mask = []\n",
    "test_news_text_mask = []\n",
    "\n",
    "# news title\n",
    "train_news_title_tensors = []\n",
    "cv_news_title_tensors = []\n",
    "test_news_title_tensors = []\n",
    "train_news_title_mask = []\n",
    "cv_news_title_mask = []\n",
    "test_news_title_mask = []\n",
    "\n",
    "# news numeric\n",
    "train_news_numeric_tensors = []\n",
    "cv_news_numeric_tensors = []\n",
    "test_news_numeric_tensors = []\n",
    "\n",
    "for idx, row in st_train_df.iterrows():\n",
    "    news_id = row['news_id']\n",
    "    news_row_id = news_df[news_df['id'] == news_id].index[0]\n",
    "    \n",
    "    train_news_text_tensors.append(news_text_padded[news_row_id])\n",
    "    train_news_text_mask.append(news_text_padded[news_row_id])\n",
    "    train_news_title_tensors.append(news_title_padded[news_row_id])\n",
    "    train_news_title_mask.append(news_title_padded[news_row_id])\n",
    "    \n",
    "    train_news_numeric_tensors.append(news_df[NEWS_NUMERIC_COLUMNS].iloc[news_row_id].values)\n",
    "    \n",
    "for idx, row in st_cv_df.iterrows():\n",
    "    news_id = row['news_id']\n",
    "    news_row_id = news_df[news_df['id'] == news_id].index[0]\n",
    "    \n",
    "    cv_news_text_tensors.append(news_text_padded[news_row_id])\n",
    "    cv_news_text_mask.append(news_text_padded[news_row_id])\n",
    "    cv_news_title_tensors.append(news_title_padded[news_row_id])\n",
    "    cv_news_title_mask.append(news_title_padded[news_row_id])\n",
    "    \n",
    "    cv_news_numeric_tensors.append(news_df[NEWS_NUMERIC_COLUMNS].iloc[news_row_id].values)\n",
    "    \n",
    "for idx, row in st_test_df.iterrows():\n",
    "    news_id = row['news_id']\n",
    "    news_row_id = news_df[news_df['id'] == news_id].index[0]\n",
    "    \n",
    "    test_news_text_tensors.append(news_text_padded[news_row_id])\n",
    "    test_news_text_mask.append(news_text_padded[news_row_id])\n",
    "    test_news_title_tensors.append(news_title_padded[news_row_id])\n",
    "    test_news_title_mask.append(news_title_padded[news_row_id])\n",
    "    \n",
    "    test_news_numeric_tensors.append(news_df[NEWS_NUMERIC_COLUMNS].iloc[news_row_id].values)\n",
    "    \n",
    "train_news_text_tensors = torch.tensor(train_news_text_tensors, dtype=torch.long).to(device)\n",
    "cv_news_text_tensors = torch.tensor(cv_news_text_tensors, dtype=torch.long).to(device)\n",
    "test_news_text_tensors = torch.tensor(test_news_text_tensors, dtype=torch.long).to(device)\n",
    "train_news_text_mask = torch.tensor(train_news_text_mask, dtype=torch.long).to(device)\n",
    "cv_news_text_mask = torch.tensor(cv_news_text_mask, dtype=torch.long).to(device)\n",
    "test_news_text_mask = torch.tensor(test_news_text_mask, dtype=torch.long).to(device)\n",
    "\n",
    "train_news_title_tensors = torch.tensor(train_news_title_tensors, dtype=torch.long).to(device)\n",
    "cv_news_title_tensors = torch.tensor(cv_news_title_tensors, dtype=torch.long).to(device)\n",
    "test_news_title_tensors = torch.tensor(test_news_title_tensors, dtype=torch.long).to(device)\n",
    "train_news_title_mask = torch.tensor(train_news_title_mask, dtype=torch.long).to(device)\n",
    "cv_news_title_mask = torch.tensor(cv_news_title_mask, dtype=torch.long).to(device)\n",
    "test_news_title_mask = torch.tensor(test_news_title_mask, dtype=torch.long).to(device)\n",
    "\n",
    "train_news_numeric_tensors = torch.tensor(train_news_numeric_tensors, dtype=torch.long).to(device)\n",
    "cv_news_numeric_tensors = torch.tensor(cv_news_numeric_tensors, dtype=torch.long).to(device)\n",
    "test_news_numeric_tensors = torch.tensor(test_news_numeric_tensors, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source Tweet (Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize\n",
    "post_train_tokenized = st_train_df['text'].apply(lambda x: tokenizer.encode(x, add_special_tokens = True))\n",
    "post_cv_tokenized = st_cv_df['text'].apply(lambda x: tokenizer.encode(x, add_special_tokens = True))\n",
    "post_test_tokenized = st_test_df['text'].apply(lambda x: tokenizer.encode(x, add_special_tokens = True))\n",
    "\n",
    "print(f\"train tokenized shape: {post_train_tokenized.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "for i in post_train_tokenized.values:\n",
    "    lengths.append(len(i))\n",
    "for i in post_cv_tokenized.values:\n",
    "    lengths.append(len(i))\n",
    "for i in post_test_tokenized.values:\n",
    "    lengths.append(len(i))\n",
    "    \n",
    "median_length = np.median(np.array(lengths))\n",
    "print(median_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we set a fixed text length, which is between max sentence length and average length\n",
    "TEXT_LENGTH = int(median_length/2)\n",
    "\n",
    "# truncate description using average length\n",
    "for i in range(len(post_train_tokenized)):\n",
    "    post_train_tokenized[i] = post_train_tokenized[i][:TEXT_LENGTH]\n",
    "for i in range(len(post_cv_tokenized)):\n",
    "    post_cv_tokenized[i] = post_cv_tokenized[i][:TEXT_LENGTH]\n",
    "for i in range(len(post_test_tokenized)):\n",
    "    post_test_tokenized[i] = post_test_tokenized[i][:TEXT_LENGTH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding\n",
    "post_train_padded = np.array([i + [0] * (TEXT_LENGTH - len(i)) for i in post_train_tokenized.values])\n",
    "post_cv_padded = np.array([i + [0] * (TEXT_LENGTH - len(i)) for i in post_cv_tokenized.values])\n",
    "post_test_padded = np.array([i + [0] * (TEXT_LENGTH - len(i)) for i in post_test_tokenized.values])\n",
    "\n",
    "print(f\"train_padded: {post_train_padded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masking\n",
    "post_train_attention_mask = np.where(post_train_padded != 0, 1, 0)\n",
    "post_cv_attention_mask = np.where(post_cv_padded != 0, 1, 0)\n",
    "post_test_attention_mask = np.where(post_test_padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put into GPU\n",
    "train_post_tensors = torch.tensor(post_train_padded, dtype=torch.long).to(device)\n",
    "cv_post_tensors = torch.tensor(post_cv_padded, dtype=torch.long).to(device)\n",
    "test_post_tensors = torch.tensor(post_test_padded, dtype=torch.long).to(device)\n",
    "\n",
    "train_post_mask = torch.tensor(post_train_attention_mask, dtype=torch.long).to(device)\n",
    "cv_post_mask = torch.tensor(post_cv_attention_mask, dtype=torch.long).to(device)\n",
    "test_post_mask = torch.tensor(post_test_attention_mask, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Profile (Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_des_tokenized = train_df['description'].apply(lambda x: tokenizer.encode(x, add_special_tokens = True))\n",
    "cv_des_tokenized = cv_df['description'].apply(lambda x: tokenizer.encode(x, add_special_tokens = True))\n",
    "test_des_tokenized = test_df['description'].apply(lambda x: tokenizer.encode(x, add_special_tokens = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_length = 0\n",
    "\n",
    "for i in train_des_tokenized.values:\n",
    "    total_length += len(i)\n",
    "for i in cv_des_tokenized.values:\n",
    "    total_length += len(i)\n",
    "for i in test_des_tokenized.values:\n",
    "    total_length += len(i)\n",
    "    \n",
    "average_length = int(total_length / (train_des_tokenized.shape[0] + cv_des_tokenized.shape[0] + test_des_tokenized.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate description using average length\n",
    "for i in range(len(train_des_tokenized)):\n",
    "    train_des_tokenized[i] = train_des_tokenized[i][:average_length]\n",
    "for i in range(len(cv_des_tokenized)):\n",
    "    cv_des_tokenized[i] = cv_des_tokenized[i][:average_length]\n",
    "for i in range(len(test_des_tokenized)):\n",
    "    test_des_tokenized[i] = test_des_tokenized[i][:average_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_des_padded = np.array([i + [0] * (average_length - len(i)) for i in train_des_tokenized.values])\n",
    "cv_des_padded = np.array([i + [0] * (average_length - len(i)) for i in cv_des_tokenized.values])\n",
    "test_des_padded = np.array([i + [0] * (average_length - len(i)) for i in test_des_tokenized.values])\n",
    "\n",
    "print(f\"train_padded: {train_des_padded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # masking\n",
    "train_attention_mask = np.where(train_des_padded != 0, 1, 0)\n",
    "cv_attention_mask = np.where(cv_des_padded != 0, 1, 0)\n",
    "test_attention_mask = np.where(test_des_padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put into GPU\n",
    "train_text_tensors = torch.tensor(train_des_padded, dtype=torch.long).to(device)\n",
    "cv_text_tensors = torch.tensor(cv_des_padded, dtype=torch.long).to(device)\n",
    "test_text_tensors = torch.tensor(test_des_padded, dtype=torch.long).to(device)\n",
    "\n",
    "train_text_mask = torch.tensor(train_attention_mask, dtype=torch.long).to(device)\n",
    "cv_text_mask = torch.tensor(cv_attention_mask, dtype=torch.long).to(device)\n",
    "test_text_mask = torch.tensor(test_attention_mask, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Source Tweet (Numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWS_NUMERIC_COLUMNS = ['img_count']\n",
    "\n",
    "# news numeric\n",
    "train_news_numeric_tensors = []\n",
    "cv_news_numeric_tensors = []\n",
    "test_news_numeric_tensors = []\n",
    "\n",
    "for idx, row in st_train_df.iterrows():\n",
    "    news_id = row['news_id']\n",
    "    news_row_id = news_df[news_df['id'] == news_id].index[0]\n",
    "    \n",
    "    train_news_numeric_tensors.append(news_df[NEWS_NUMERIC_COLUMNS].iloc[news_row_id].values)\n",
    "    \n",
    "for idx, row in st_cv_df.iterrows():\n",
    "    news_id = row['news_id']\n",
    "    news_row_id = news_df[news_df['id'] == news_id].index[0]\n",
    "    \n",
    "    cv_news_numeric_tensors.append(news_df[NEWS_NUMERIC_COLUMNS].iloc[news_row_id].values)\n",
    "    \n",
    "for idx, row in st_test_df.iterrows():\n",
    "    news_id = row['news_id']\n",
    "    news_row_id = news_df[news_df['id'] == news_id].index[0]\n",
    "    \n",
    "    test_news_numeric_tensors.append(news_df[NEWS_NUMERIC_COLUMNS].iloc[news_row_id].values)\n",
    "    \n",
    "\n",
    "train_news_numeric_tensors = torch.tensor(train_news_numeric_tensors, dtype=torch.long).to(device)\n",
    "cv_news_numeric_tensors = torch.tensor(cv_news_numeric_tensors, dtype=torch.long).to(device)\n",
    "test_news_numeric_tensors = torch.tensor(test_news_numeric_tensors, dtype=torch.long).to(device)\n",
    "\n",
    "print(train_news_numeric_tensors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistic_columns = ['user_count', 'tag_count', 'symbol_count', 'url_count', 'sentence_count']\n",
    "\n",
    "wds = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
    "temporal_columns = [f\"h_{h:02}\" for h in range(0,24)] + [f\"wday_{wd}\" for wd in wds] + ['is_holiday']\n",
    "\n",
    "sentiment_columns = ['avg_sentiment_score', 'sentiment_ratio', 'pos_count', 'neg_count']\n",
    "\n",
    "post_numeric_columns = statistic_columns + temporal_columns + sentiment_columns\n",
    "print(post_numeric_columns)\n",
    "\n",
    "train_post_numeric_tensors = torch.tensor(st_train_df[post_numeric_columns].values, dtype=torch.float).to(device)\n",
    "cv_post_numeric_tensors = torch.tensor(st_cv_df[post_numeric_columns].values, dtype=torch.float).to(device)\n",
    "test_post_numeric_tensors = torch.tensor(st_test_df[post_numeric_columns].values, dtype=torch.float).to(device)\n",
    "\n",
    "print(train_post_numeric_tensors.shape)\n",
    "print(cv_post_numeric_tensors.shape)\n",
    "print(test_post_numeric_tensors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User Profile (Numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "up_numeric_columns = ['followers_count', 'friends_count', 'listed_count', 'favorites_count', 'statuses_count']\n",
    "up_boolean_columns = ['protected', 'geo_enabled', 'verified']\n",
    "\n",
    "# convert boolean values to numeric values\n",
    "d = {True: 1, False: 0}\n",
    "for c in up_boolean_columns:\n",
    "    up_train_df[c] = up_train_df[c].map(d)\n",
    "    up_cv_df[c] = up_cv_df[c].map(d)\n",
    "    up_test_df[c] = up_test_df[c].map(d)\n",
    "\n",
    "# create tensors in GPU\n",
    "train_up_numeric_tensors = torch.tensor(up_train_df[up_boolean_columns+up_numeric_columns].values, dtype=torch.float).to(device)\n",
    "cv_up_numeric_tensors = torch.tensor(up_cv_df[up_boolean_columns+up_numeric_columns].values, dtype=torch.float).to(device)\n",
    "test_up_numeric_tensors = torch.tensor(up_test_df[up_boolean_columns+up_numeric_columns].values, dtype=torch.float).to(device)\n",
    "\n",
    "print(train_up_numeric_tensors.shape)\n",
    "print(cv_up_numeric_tensors.shape)\n",
    "print(test_up_numeric_tensors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Timeline (Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_tokenized = train_df['text'].apply(lambda x: tokenizer.encode(x, add_special_tokens = True))\n",
    "cv_text_tokenized = cv_df['text'].apply(lambda x: tokenizer.encode(x, add_special_tokens = True))\n",
    "test_text_tokenized = test_df['text'].apply(lambda x: tokenizer.encode(x, add_special_tokens = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # truncate description using average length\n",
    "for i in range(len(train_text_tokenized)):\n",
    "    train_text_tokenized[i] = train_text_tokenized[i][:average_length]\n",
    "for i in range(len(cv_text_tokenized)):\n",
    "    cv_text_tokenized[i] = cv_text_tokenized[i][:average_length]\n",
    "for i in range(len(test_text_tokenized)):\n",
    "    test_text_tokenized[i] = test_text_tokenized[i][:average_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding\n",
    "train_text_padded = np.array([i + [0] * (average_length - len(i)) for i in train_text_tokenized.values])\n",
    "cv_text_padded = np.array([i + [0] * (average_length - len(i)) for i in cv_text_tokenized.values])\n",
    "test_text_padded = np.array([i + [0] * (average_length - len(i)) for i in test_text_tokenized.values])\n",
    "\n",
    "print(f\"train_padded: {train_text_padded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # masking\n",
    "train_attention_mask = np.where(train_text_padded != 0, 1, 0)\n",
    "cv_attention_mask = np.where(cv_text_padded != 0, 1, 0)\n",
    "test_attention_mask = np.where(test_text_padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put into GPU\n",
    "train_text_tensors = torch.tensor(train_text_padded, dtype=torch.long).to(device)\n",
    "cv_text_tensors = torch.tensor(cv_text_padded, dtype=torch.long).to(device)\n",
    "test_text_tensors = torch.tensor(test_text_padded, dtype=torch.long).to(device)\n",
    "\n",
    "train_text_mask = torch.tensor(train_attention_mask, dtype=torch.long).to(device)\n",
    "cv_text_mask = torch.tensor(cv_attention_mask, dtype=torch.long).to(device)\n",
    "test_text_mask = torch.tensor(test_attention_mask, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User Timeline (Numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut_numeric_columns = ['avg_rt_count', 'avg_favorite_count', 'sensitive_ratio',\n",
    "                   'sentence_count', 'avg_sentiment_score', 'sentiment_ratio', 'pos_count', 'neg_count']\n",
    "\n",
    "train_ut_numeric_tensors = torch.tensor(ut_train_df[ut_numeric_columns].values, dtype=torch.float).to(device)\n",
    "cv_ut_numeric_tensors = torch.tensor(ut_cv_df[ut_numeric_columns].values, dtype=torch.float).to(device)\n",
    "test_ut_numeric_tensors = torch.tensor(ut_test_df[ut_numeric_columns].values, dtype=torch.float).to(device)\n",
    "\n",
    "print(train_ut_numeric_tensors.shape)\n",
    "print(cv_ut_numeric_tensors.shape)\n",
    "print(test_ut_numeric_tensors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cascade Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_h1_graphs = []\n",
    "train_h2_graphs = []\n",
    "train_h3_graphs = []\n",
    "\n",
    "cv_h1_graphs = []\n",
    "cv_h2_graphs = []\n",
    "cv_h3_graphs = []\n",
    "\n",
    "test_h1_graphs = []\n",
    "test_h2_graphs = []\n",
    "test_h3_graphs = []\n",
    "\n",
    "for df_idx, df in enumerate([cn_train_df, cn_cv_df, cn_test_df]):\n",
    "    \n",
    "    for row_idx, row in df.iterrows():\n",
    "        \n",
    "        graph_series = []\n",
    "        \n",
    "        h1_nodes = json.loads(row['h1_nodes'].replace(\"'\", '\"'))\n",
    "        h1_edges = json.loads(row['h1_edges'].replace(\"'\", '\"'))\n",
    "        h2_nodes = json.loads(row['h2_nodes'].replace(\"'\", '\"'))\n",
    "        h2_edges = json.loads(row['h2_edges'].replace(\"'\", '\"'))\n",
    "        h3_nodes = json.loads(row['h3_nodes'].replace(\"'\", '\"'))\n",
    "        h3_edges = json.loads(row['h3_edges'].replace(\"'\", '\"'))\n",
    "        \n",
    "        for (nodes, edges) in [(h1_nodes, h1_edges), (h2_nodes, h2_edges), (h3_nodes, h3_edges)]:\n",
    "    \n",
    "            # do re-index & add nodes\n",
    "            node_reindex_map = {}\n",
    "            node_features = []\n",
    "            current_id = 0\n",
    "            for node_id in list(nodes.keys()):\n",
    "                node_features.append(nodes[node_id])\n",
    "                node_reindex_map[int(node_id)] = current_id\n",
    "                current_id += 1\n",
    "\n",
    "            # add edges\n",
    "            nodes_from = []\n",
    "            nodes_to = []\n",
    "            for edge in edges:\n",
    "                # edge: [user A, user A's follower]\n",
    "                nodes_from.append(node_reindex_map[edge[0]])\n",
    "                nodes_to.append(node_reindex_map[edge[1]])\n",
    "\n",
    "            x = torch.tensor(node_features, dtype=torch.float)\n",
    "            edge_index = torch.tensor([nodes_from, nodes_to], dtype=torch.long)\n",
    "\n",
    "            graph = Data(x=x, edge_index=edge_index).to(device)\n",
    "            graph_series.append(graph)\n",
    "    \n",
    "    \n",
    "        if df_idx == 0:\n",
    "            train_h1_graphs.append(graph_series[0])\n",
    "            train_h2_graphs.append(graph_series[1])\n",
    "            train_h3_graphs.append(graph_series[2])\n",
    "        elif df_idx == 1:\n",
    "            cv_h1_graphs.append(graph_series[0])\n",
    "            cv_h2_graphs.append(graph_series[1])\n",
    "            cv_h3_graphs.append(graph_series[2])\n",
    "        else:\n",
    "            test_h1_graphs.append(graph_series[0])\n",
    "            test_h2_graphs.append(graph_series[1])\n",
    "            test_h3_graphs.append(graph_series[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 25000\n",
    "\n",
    "train_h1_dataset = DataLoader(train_h1_graphs, batch_size=BATCH_SIZE, shuffle=False)\n",
    "train_h2_dataset = DataLoader(train_h2_graphs, batch_size=BATCH_SIZE, shuffle=False)\n",
    "train_h3_dataset = DataLoader(train_h3_graphs, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "cv_h1_dataset = DataLoader(cv_h1_graphs, batch_size=BATCH_SIZE, shuffle=False)\n",
    "cv_h2_dataset = DataLoader(cv_h2_graphs, batch_size=BATCH_SIZE, shuffle=False)\n",
    "cv_h3_dataset = DataLoader(cv_h3_graphs, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "test_h1_dataset = DataLoader(test_h1_graphs, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_h2_dataset = DataLoader(test_h2_graphs, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_h3_dataset = DataLoader(test_h3_graphs, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CN_NUMERIC_COLUMNS = ['max_deg', 'avg_deg', 'min_deg', 'max_timediff', 'min_timediff', 'avg_timediff', 'node_number', 'edge_number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cascade_numeric_tensors = torch.tensor(cn_train_df[CN_NUMERIC_COLUMNS].values, dtype=torch.float).to(device)\n",
    "cv_cascade_numeric_tensors = torch.tensor(cn_cv_df[CN_NUMERIC_COLUMNS].values, dtype=torch.float).to(device)\n",
    "test_cascade_numeric_tensors = torch.tensor(cn_test_df[CN_NUMERIC_COLUMNS].values, dtype=torch.float).to(device)\n",
    "\n",
    "print(train_cascade_numeric_tensors.shape)\n",
    "print(cv_cascade_numeric_tensors.shape)\n",
    "print(test_cascade_numeric_tensors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.tensor(ut_train_df['cascade_size'].values, dtype=torch.float).unsqueeze(1).to(device)\n",
    "cv_labels = torch.tensor(ut_cv_df['cascade_size'].values, dtype=torch.float).unsqueeze(1).to(device)\n",
    "test_labels = torch.tensor(ut_test_df['cascade_size'].values, dtype=torch.float).unsqueeze(1).to(device)\n",
    "\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention network\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, feature_dim, drop_ratio=0):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(feature_dim, feature_dim)\n",
    "        self.linear2 = nn.Linear(feature_dim, feature_dim)\n",
    "        self.dropout = nn.Dropout(p = drop_ratio)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # input shape: batch_size x feature_dim\n",
    "        \n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        weights = F.softmax(x, dim=1)\n",
    "\n",
    "        return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PopularityModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, node_dimension=8, numeric_dimension = (41+1+8+8+8)):\n",
    "        \n",
    "        super(PopularityModel, self).__init__()\n",
    "        \n",
    "        # dimensions setup\n",
    "        self.node_dim = node_dimension\n",
    "        self.numeric_dim = numeric_dimension\n",
    "        self.rnn_hidden_dim = int(self.node_dim/2)\n",
    "        self.mlp_dim = self.rnn_hidden_dim + self.numeric_dim\n",
    "        \n",
    "        # GCN layers\n",
    "        self.g1_gconv = GCNConv(self.node_dim, self.node_dim)\n",
    "        self.g2_gconv = GCNConv(self.node_dim, self.node_dim)\n",
    "        self.g3_gconv = GCNConv(self.node_dim, self.node_dim)\n",
    "        \n",
    "        # RNN\n",
    "        self.rnn = nn.GRUCell(self.node_dim, int(self.node_dim/2))\n",
    "        \n",
    "        self.attention = AttentionLayer(self.mlp_dim, drop_ratio=0)\n",
    "        \n",
    "        # final MLP layers\n",
    "        self.linear1 = nn.Linear(self.mlp_dim, int(self.mlp_dim/2))\n",
    "        self.linear2 = nn.Linear(int(self.mlp_dim/2), int(self.mlp_dim/4))\n",
    "        self.linear3 = nn.Linear(int(self.mlp_dim/4), 1)\n",
    "        \n",
    "    def forward(self, graph1, graph2, graph3, numeric_features):\n",
    "        \n",
    "        batch_size = graph1.num_graphs\n",
    "        \n",
    "        g1_x, g1_edge_index = graph1.x, graph1.edge_index\n",
    "        g2_x, g2_edge_index = graph2.x, graph2.edge_index\n",
    "        g3_x, g3_edge_index = graph3.x, graph3.edge_index\n",
    "        \n",
    "        g1_x = F.relu(self.g1_gconv(g1_x, g1_edge_index)) # shape: (all node_number) * node_dimension\n",
    "        g1_x_pooled = torch.zeros((batch_size, self.node_dim)).to(device) # shape: batch_size * node_dimension\n",
    "        for g_idx in range(batch_size):\n",
    "            indexs = torch.nonzero(graph1.batch == g_idx).squeeze(1)\n",
    "            g1_x_pooled[g_idx] = torch.sum(g1_x[indexs], 0)\n",
    "            \n",
    "        g2_x = F.relu(self.g2_gconv(g2_x, g2_edge_index)) # shape: (all node_number) * node_dimension\n",
    "        g2_x_pooled = torch.zeros((batch_size, self.node_dim)).to(device) # shape: batch_size * node_dimension\n",
    "        for g_idx in range(batch_size):\n",
    "            indexs = torch.nonzero(graph2.batch == g_idx).squeeze(1)\n",
    "            g2_x_pooled[g_idx] = torch.sum(g2_x[indexs], 0)\n",
    "            \n",
    "        g3_x = F.relu(self.g3_gconv(g3_x, g3_edge_index)) # shape: (all node_number) * node_dimension\n",
    "        g3_x_pooled = torch.zeros((batch_size, self.node_dim)).to(device) # shape: batch_size * node_dimension\n",
    "        for g_idx in range(batch_size):\n",
    "            indexs = torch.nonzero(graph3.batch == g_idx).squeeze(1)\n",
    "            g3_x_pooled[g_idx] = torch.sum(g3_x[indexs], 0)\n",
    "        \n",
    "        hx = torch.randn((batch_size, self.rnn_hidden_dim), dtype=torch.float).to(device)\n",
    "        hx = self.rnn(g1_x_pooled, hx)\n",
    "        hx = self.rnn(g2_x_pooled, hx)\n",
    "        hx = self.rnn(g3_x_pooled, hx)\n",
    "        \n",
    "        concatenated = torch.cat((hx, numeric_features), 1)\n",
    "        \n",
    "        attention_weights = self.attention(concatenated)\n",
    "        \n",
    "        x = concatenated * attention_weights\n",
    "\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        y = F.relu(self.linear3(x))\n",
    "\n",
    "        return (y, attention_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_at_k(predicted, labels, k = 10):\n",
    "    \n",
    "    # check whether both sizes are identical\n",
    "    assert predicted.size(0) == labels.size(0)\n",
    "    \n",
    "    # sort the values in descending order and gets the indexs\n",
    "    sorted_predicted_index = torch.argsort(predicted, descending = True)\n",
    "    sorted_label_index = torch.argsort(labels, descending = True)\n",
    "    \n",
    "    k_number = max(int(predicted.size(0) * k / 100), 1)\n",
    "    \n",
    "    topk_predicted_index = sorted_predicted_index[:k_number]\n",
    "    topk_label_index = sorted_label_index[:k_number]\n",
    "    \n",
    "    hit_count = 0\n",
    "    for p in topk_predicted_index:\n",
    "        if p in topk_label_index:\n",
    "            hit_count += 1\n",
    "            \n",
    "    accuracy = hit_count/k_number\n",
    "            \n",
    "    return (accuracy, hit_count, k_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_REGRESSION = 4e-3\n",
    "EPOCH = 200\n",
    "EARLY_STOP_PATIENCE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_post_numeric_tensors + train_news_numeric_tensors + train_up_numeric_tensors + train_ut_numeric_tensors + train_cascade_numeric_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(regression_lr=LR_REGRESSION, max_epoch=EPOCH, early_stop_patience=EARLY_STOP_PATIENCE, verbose=True, manual_seed=None):\n",
    "    \n",
    "    if manual_seed:\n",
    "        seed = manual_seed\n",
    "    else:\n",
    "        seed = torch.random.seed()\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    popularity_model = PopularityModel(node_dimension=8, numeric_dimension = (41+1+8+8+8)).to(device)\n",
    "    popularity_model.train()\n",
    "\n",
    "    optimizer_regression = torch.optim.Adam(popularity_model.parameters(), lr = regression_lr)\n",
    "    \n",
    "    epoch_losses = []\n",
    "\n",
    "    # cross validation for early stopping\n",
    "    current_val_error = float('inf')\n",
    "    val_error_inc_count = 0\n",
    "    cv_losses = []\n",
    "    \n",
    "    for epoch in range(max_epoch):\n",
    "    \n",
    "        if verbose:\n",
    "            print(epoch, end=\"\")\n",
    "\n",
    "        batch_losses = []\n",
    "        \n",
    "        train_h1_it = iter(train_h1_dataset)\n",
    "        train_h2_it = iter(train_h2_dataset)\n",
    "        train_h3_it = iter(train_h3_dataset)\n",
    "\n",
    "        # batch training\n",
    "        for i in range(0, train_labels.size(0), BATCH_SIZE):\n",
    "            \n",
    "            if verbose:\n",
    "                print(\".\", end=\"\")\n",
    "\n",
    "            optimizer_regression.zero_grad()\n",
    "\n",
    "            END = (i + BATCH_SIZE) if (i + BATCH_SIZE) < train_labels.size(0) else train_labels.size(0)\n",
    "            \n",
    "            graph_h1, graph_h2, graph_h3 = next(train_h1_it), next(train_h2_it), next(train_h3_it)\n",
    "            graph_h1.to(device)\n",
    "            graph_h2.to(device)\n",
    "            graph_h3.to(device)\n",
    "            \n",
    "            batch_numeric_features = torch.cat((\n",
    "                train_post_numeric_tensors[i:END], # dim=41\n",
    "                train_news_numeric_tensors[i:END], # dim=1\n",
    "                train_up_numeric_tensors[i:END], # dim=8\n",
    "                train_ut_numeric_tensors[i:END], # dim=8\n",
    "                train_cascade_numeric_tensors[i:END] # dim=8\n",
    "            ), 1)\n",
    "            \n",
    "            batch_labels = train_labels[i:END]\n",
    "\n",
    "            # forward: GNN+RNN+Linear Regression\n",
    "            predicted, attn_weights = popularity_model(graph_h1, graph_h2, graph_h3, batch_numeric_features)\n",
    "\n",
    "            # compute loss (weighted mean squared error)\n",
    "            loss = F.mse_loss(predicted, batch_labels, reduction='mean')\n",
    "\n",
    "            # backward propagation\n",
    "            loss.backward()\n",
    "            optimizer_regression.step()\n",
    "\n",
    "            batch_losses.append(loss)\n",
    "\n",
    "        epoch_loss = torch.tensor(batch_losses).mean().item()\n",
    "        if verbose:\n",
    "            print(f\"Loss: {epoch_loss:.4f}\", end=\",\\n\")\n",
    "        \n",
    "        if (epoch > 0) and (epoch_loss == epoch_losses[-1]):\n",
    "            if verbose:\n",
    "                print(f\"early stopping triggered! stopped at epoch {epoch}\")\n",
    "                break\n",
    "                \n",
    "        epoch_losses.append(epoch_loss)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "        \n",
    "            batch_losses = []\n",
    "            \n",
    "            cv_h1_it = iter(cv_h1_dataset)\n",
    "            cv_h2_it = iter(cv_h2_dataset)\n",
    "            cv_h3_it = iter(cv_h3_dataset)\n",
    "            \n",
    "            for i in range(0, cv_labels.size(0), BATCH_SIZE):\n",
    "\n",
    "                END = (i + BATCH_SIZE) if (i + BATCH_SIZE) < cv_labels.size(0) else cv_labels.size(0)\n",
    "                \n",
    "                graph_h1, graph_h2, graph_h3 = next(cv_h1_it), next(cv_h2_it), next(cv_h3_it)\n",
    "                graph_h1.to(device)\n",
    "                graph_h2.to(device)\n",
    "                graph_h3.to(device)\n",
    "                \n",
    "                batch_numeric_features = torch.cat((\n",
    "                    cv_post_numeric_tensors[i:END], # dim=41\n",
    "                    cv_news_numeric_tensors[i:END], # dim=1\n",
    "                    cv_up_numeric_tensors[i:END], # dim=8\n",
    "                    cv_ut_numeric_tensors[i:END], # dim=8\n",
    "                    cv_cascade_numeric_tensors[i:END] # dim=8\n",
    "                ), 1)\n",
    "                \n",
    "                batch_labels = cv_labels[i:END]\n",
    "\n",
    "                # forward: GNN+RNN+Linear Regression\n",
    "                predicted, attn_weights = popularity_model(graph_h1, graph_h2, graph_h3, batch_numeric_features)\n",
    "\n",
    "                # compute loss (weighted mean squared error)\n",
    "                loss = F.mse_loss(predicted, batch_labels, reduction='mean')\n",
    "\n",
    "                batch_losses.append(loss)\n",
    "\n",
    "            cv_error = torch.tensor(batch_losses).mean().item()\n",
    "            cv_losses.append(cv_error)\n",
    "\n",
    "            if cv_error >= current_val_error:\n",
    "                val_error_inc_count += 1\n",
    "                current_val_error = cv_error\n",
    "                if val_error_inc_count >= early_stop_patience:\n",
    "                    if verbose:\n",
    "                        print(f\"early stopping triggered! stopped at epoch {epoch}\")\n",
    "                    break\n",
    "            else:\n",
    "                val_error_inc_count = 0\n",
    "                current_val_error = cv_error\n",
    "    \n",
    "    # evaluation\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        test_h1_it = iter(test_h1_dataset)\n",
    "        test_h2_it = iter(test_h2_dataset)\n",
    "        test_h3_it = iter(test_h3_dataset)\n",
    "\n",
    "        model_test_predicted = torch.zeros((test_labels.size(0),), dtype=torch.float).to(device)\n",
    "        feature_attention_weights = torch.zeros((test_labels.size(0), popularity_model.mlp_dim), dtype=torch.float).to(device)\n",
    "        for i in range(0, test_labels.size(0), BATCH_SIZE):\n",
    "\n",
    "            END = (i + BATCH_SIZE) if (i + BATCH_SIZE) < test_labels.size(0) else test_labels.size(0)\n",
    "            \n",
    "            graph_h1, graph_h2, graph_h3 = next(test_h1_it), next(test_h2_it), next(test_h3_it)\n",
    "            graph_h1.to(device)\n",
    "            graph_h2.to(device)\n",
    "            graph_h3.to(device)\n",
    "            \n",
    "            batch_numeric_features = torch.cat((\n",
    "                test_post_numeric_tensors[i:END], # dim=41\n",
    "                test_news_numeric_tensors[i:END], # dim=1\n",
    "                test_up_numeric_tensors[i:END], # dim=8\n",
    "                test_ut_numeric_tensors[i:END], # dim=8\n",
    "                test_cascade_numeric_tensors[i:END] # dim=8\n",
    "            ), 1)\n",
    "            batch_labels = test_labels[i:END]\n",
    "\n",
    "            # forward: GNN+RNN+Linear Regression\n",
    "            predicted, attn_weights = popularity_model(graph_h1, graph_h2, graph_h3, batch_numeric_features)\n",
    "            model_test_predicted[i:END] = predicted.squeeze(1)\n",
    "            feature_attention_weights[i:END] = attn_weights\n",
    "            \n",
    "        # get average attention weights for every feature\n",
    "        avg_feature_importance = torch.mean(feature_attention_weights, 0)\n",
    "        print(\"Averaged Feature Importance:\")\n",
    "        print(avg_feature_importance)\n",
    "\n",
    "        testset_size = test_labels.size(0)\n",
    "        \n",
    "        # record attention weights individually\n",
    "        importance_records = []\n",
    "        for idx in range(testset_size):\n",
    "            label = test_labels[idx].item()\n",
    "            if label > 95:\n",
    "                importance_records.append({\n",
    "                    'idx': idx,\n",
    "                    'label': label,\n",
    "                    'attention_weights': feature_attention_weights[idx].tolist()\n",
    "                })\n",
    "        with open(f\"./records/attention_weights_{seed}\", 'w') as f:\n",
    "            f.write(json.dumps(importance_records))\n",
    "\n",
    "        model_mae_scores = F.l1_loss(model_test_predicted, test_labels)\n",
    "        model_mse_scores = F.mse_loss(model_test_predicted, test_labels)\n",
    "\n",
    "        hit_rate_top1p = accuracy_at_k(model_test_predicted, test_labels.squeeze(1), 1)\n",
    "        hit_rate_top5p = accuracy_at_k(model_test_predicted, test_labels.squeeze(1), 5)\n",
    "        hit_rate_top10p = accuracy_at_k(model_test_predicted, test_labels.squeeze(1), 10)\n",
    "        hit_rate_top15p = accuracy_at_k(model_test_predicted, test_labels.squeeze(1), 15)\n",
    "\n",
    "        ndcg_score_1p = sklearn.metrics.ndcg_score(test_labels.reshape((1, -1)).cpu(), model_test_predicted.unsqueeze(0).cpu(), k=int(testset_size * 1 / 100))\n",
    "        ndcg_score_5p = sklearn.metrics.ndcg_score(test_labels.reshape((1, -1)).cpu(), model_test_predicted.unsqueeze(0).cpu(), k=int(testset_size * 5 / 100))\n",
    "        ndcg_score_10p = sklearn.metrics.ndcg_score(test_labels.reshape((1, -1)).cpu(), model_test_predicted.unsqueeze(0).cpu(), k=int(testset_size * 10 / 100))\n",
    "        ndcg_score_15p = sklearn.metrics.ndcg_score(test_labels.reshape((1, -1)).cpu(), model_test_predicted.unsqueeze(0).cpu(), k=int(testset_size * 15 / 100))\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"seed: {seed}\")\n",
    "            print(f\"MAE: {model_mae_scores.item()}\")\n",
    "            print(f\"MSE: {model_mse_scores.item()}\")\n",
    "            print(f\"Hit Rate@1%: {hit_rate_top1p}\")\n",
    "            print(f\"Hit Rate@5%: {hit_rate_top5p}\")\n",
    "            print(f\"Hit Rate@10%: {hit_rate_top10p}\")\n",
    "            print(f\"Hit Rate@15%: {hit_rate_top15p}\")\n",
    "            print(f\"NDCG@1%: {ndcg_score_1p}\")\n",
    "            print(f\"NDCG@5%: {ndcg_score_5p}\")\n",
    "            print(f\"NDCG@10%: {ndcg_score_10p}\")\n",
    "            print(f\"NDCG@15%: {ndcg_score_15p}\")\n",
    "            \n",
    "            plt.plot(epoch_losses, label = 'training')\n",
    "            plt.plot(cv_losses, label = 'validation')\n",
    "            plt.xlabel('epoch'), plt.ylabel('MSE')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "        # clear useless CUDA memory\n",
    "        popularity_model = None\n",
    "        optimizer_regression = None\n",
    "        graph_h1, graph_h2, graph_h3 = None, None, None\n",
    "        batch_numeric_features = None\n",
    "        batch_labels = None\n",
    "        predicted = None\n",
    "        loss = None\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return {\n",
    "            'seed': seed,\n",
    "            'mae': model_mae_scores.item(),\n",
    "            'mse': model_mse_scores.item(),\n",
    "            'hr1p': hit_rate_top1p[0],\n",
    "            'hr5p': hit_rate_top5p[0],\n",
    "            'hr10p': hit_rate_top10p[0],\n",
    "            'hr15p': hit_rate_top15p[0],\n",
    "            'ndcg1p': ndcg_score_1p,\n",
    "            'ndcg5p': ndcg_score_5p,\n",
    "            'ndcg10p': ndcg_score_10p,\n",
    "            'ndcg15p': ndcg_score_15p\n",
    "        };"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_PER_SETTING = 10\n",
    "results = []\n",
    "for i in range(MODELS_PER_SETTING):\n",
    "    print(f\"model {i+1} - {datetime.now()}\")\n",
    "\n",
    "    res = train(regression_lr=4e-3, max_epoch=200, early_stop_patience=2, verbose=True, manual_seed=None)\n",
    "    res['number'] = i\n",
    "    res['score'] = res['hr1p'] + res['hr5p'] + res['hr10p'] + res['hr15p'] + res['ndcg1p'] + res['ndcg5p'] + res['ndcg10p'] + res['ndcg15p']\n",
    "    print(f\"score: {res['score']}\")\n",
    "    results.append(res)\n",
    "    print(f\"model {i+1} done - {datetime.now()} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
