{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cascade Network (Numeric Features + Graph Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import transformers as ppb\n",
    "from transformers import AdamW\n",
    "\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "import os.path as path\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import sklearn\n",
    "import scipy\n",
    "\n",
    "# print messages\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device type: {device.type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_TRAIN = \"./data/splitted/cascade_network_train.csv\"\n",
    "DATASET_CV = \"./data/splitted/cascade_network_cv.csv\"\n",
    "DATASET_TEST = \"./data/splitted/cascade_network_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATASET_TRAIN, header = 0)\n",
    "cv_df = pd.read_csv(DATASET_CV, header = 0)\n",
    "test_df = pd.read_csv(DATASET_TEST, header = 0)\n",
    "\n",
    "print(f\"Columns: {train_df.columns}\")\n",
    "print(f\"train set size: {len(train_df)}\")\n",
    "print(f\"cross validation set size: {len(cv_df)}\")\n",
    "print(f\"test set size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_column_names = ['max_deg', 'avg_deg', 'min_deg', 'max_timediff', 'min_timediff', 'avg_timediff', 'node_number', 'edge_number']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_numeric_tensors = torch.tensor(train_df[numeric_column_names].values, dtype=torch.float).to(device)\n",
    "cv_numeric_tensors = torch.tensor(cv_df[numeric_column_names].values, dtype=torch.float).to(device)\n",
    "test_numeric_tensors = torch.tensor(test_df[numeric_column_names].values, dtype=torch.float).to(device)\n",
    "\n",
    "print(train_numeric_tensors.shape)\n",
    "print(cv_numeric_tensors.shape)\n",
    "print(test_numeric_tensors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_h1_graphs = []\n",
    "train_h2_graphs = []\n",
    "train_h3_graphs = []\n",
    "\n",
    "cv_h1_graphs = []\n",
    "cv_h2_graphs = []\n",
    "cv_h3_graphs = []\n",
    "\n",
    "test_h1_graphs = []\n",
    "test_h2_graphs = []\n",
    "test_h3_graphs = []\n",
    "\n",
    "for df_idx, df in enumerate([train_df, cv_df, test_df]):\n",
    "    \n",
    "    for row_idx, row in df.iterrows():\n",
    "        \n",
    "        graph_series = []\n",
    "        \n",
    "        h1_nodes = json.loads(row['h1_nodes'].replace(\"'\", '\"'))\n",
    "        h1_edges = json.loads(row['h1_edges'].replace(\"'\", '\"'))\n",
    "        h2_nodes = json.loads(row['h2_nodes'].replace(\"'\", '\"'))\n",
    "        h2_edges = json.loads(row['h2_edges'].replace(\"'\", '\"'))\n",
    "        h3_nodes = json.loads(row['h3_nodes'].replace(\"'\", '\"'))\n",
    "        h3_edges = json.loads(row['h3_edges'].replace(\"'\", '\"'))\n",
    "        \n",
    "        for (nodes, edges) in [(h1_nodes, h1_edges), (h2_nodes, h2_edges), (h3_nodes, h3_edges)]:\n",
    "    \n",
    "            # do re-index & add nodes\n",
    "            node_reindex_map = {}\n",
    "            node_features = []\n",
    "            current_id = 0\n",
    "            for node_id in list(nodes.keys()):\n",
    "                node_features.append(nodes[node_id])\n",
    "                node_reindex_map[int(node_id)] = current_id\n",
    "                current_id += 1\n",
    "\n",
    "            # add edges\n",
    "            nodes_from = []\n",
    "            nodes_to = []\n",
    "            for edge in edges:\n",
    "                # edge: [user A, user A's follower]\n",
    "                nodes_from.append(node_reindex_map[edge[0]])\n",
    "                nodes_to.append(node_reindex_map[edge[1]])\n",
    "\n",
    "            x = torch.tensor(node_features, dtype=torch.float)\n",
    "            edge_index = torch.tensor([nodes_from, nodes_to], dtype=torch.long)\n",
    "\n",
    "            graph = Data(x=x, edge_index=edge_index).to(device)\n",
    "            graph_series.append(graph)\n",
    "    \n",
    "    \n",
    "        if df_idx == 0:\n",
    "            train_h1_graphs.append(graph_series[0])\n",
    "            train_h2_graphs.append(graph_series[1])\n",
    "            train_h3_graphs.append(graph_series[2])\n",
    "        elif df_idx == 1:\n",
    "            cv_h1_graphs.append(graph_series[0])\n",
    "            cv_h2_graphs.append(graph_series[1])\n",
    "            cv_h3_graphs.append(graph_series[2])\n",
    "        else:\n",
    "            test_h1_graphs.append(graph_series[0])\n",
    "            test_h2_graphs.append(graph_series[1])\n",
    "            test_h3_graphs.append(graph_series[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10000\n",
    "\n",
    "train_h1_dataset = DataLoader(train_h1_graphs, batch_size=BATCH_SIZE, shuffle=False)\n",
    "train_h2_dataset = DataLoader(train_h2_graphs, batch_size=BATCH_SIZE, shuffle=False)\n",
    "train_h3_dataset = DataLoader(train_h3_graphs, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "cv_h1_dataset = DataLoader(cv_h1_graphs, batch_size=BATCH_SIZE, shuffle=False)\n",
    "cv_h2_dataset = DataLoader(cv_h2_graphs, batch_size=BATCH_SIZE, shuffle=False)\n",
    "cv_h3_dataset = DataLoader(cv_h3_graphs, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "test_h1_dataset = DataLoader(test_h1_graphs, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_h2_dataset = DataLoader(test_h2_graphs, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_h3_dataset = DataLoader(test_h3_graphs, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.tensor(train_df['cascade_size'].values, dtype=torch.float).unsqueeze(1).to(device)\n",
    "cv_labels = torch.tensor(cv_df['cascade_size'].values, dtype=torch.float).unsqueeze(1).to(device)\n",
    "test_labels = torch.tensor(test_df['cascade_size'].values, dtype=torch.float).unsqueeze(1).to(device)\n",
    "\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PopularityModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, node_dimension=8, numeric_dimension=8):\n",
    "        \n",
    "        super(PopularityModel, self).__init__()\n",
    "        \n",
    "        self.node_dim = node_dimension\n",
    "        self.numeric_dim = numeric_dimension\n",
    "        \n",
    "        self.mlp_dim = int(self.node_dim/2) + self.numeric_dim\n",
    "        \n",
    "        self.g1_gconv = GCNConv(self.node_dim, self.node_dim)\n",
    "        self.g2_gconv = GCNConv(self.node_dim, self.node_dim)\n",
    "        self.g3_gconv = GCNConv(self.node_dim, self.node_dim)\n",
    "        \n",
    "        self.rnn = nn.GRUCell(self.node_dim, int(self.node_dim/2))\n",
    "        \n",
    "        self.linear1 = nn.Linear(self.mlp_dim, int(self.mlp_dim/2))\n",
    "        self.linear2 = nn.Linear(int(self.mlp_dim/2), int(self.mlp_dim/4))\n",
    "        self.linear3 = nn.Linear(int(self.mlp_dim/4), 1)\n",
    "\n",
    "    def forward(self, graph1, graph2, graph3, numeric_features):\n",
    "        \n",
    "        batch_size = graph1.num_graphs\n",
    "        \n",
    "        g1_x, g1_edge_index = graph1.x, graph1.edge_index\n",
    "        g2_x, g2_edge_index = graph2.x, graph2.edge_index\n",
    "        g3_x, g3_edge_index = graph3.x, graph3.edge_index\n",
    "        \n",
    "        g1_x = F.relu(self.g1_gconv(g1_x, g1_edge_index)) # shape: (all node_number) * node_dimension\n",
    "        g1_x_pooled = torch.zeros((batch_size, self.node_dim)).to(device) # shape: batch_size * node_dimension\n",
    "        for g_idx in range(batch_size):\n",
    "            indexs = torch.nonzero(graph1.batch == g_idx).squeeze(1)\n",
    "            g1_x_pooled[g_idx] = torch.sum(g1_x[indexs], 0)\n",
    "            \n",
    "        g2_x = F.relu(self.g2_gconv(g2_x, g2_edge_index)) # shape: (all node_number) * node_dimension\n",
    "        g2_x_pooled = torch.zeros((batch_size, self.node_dim)).to(device) # shape: batch_size * node_dimension\n",
    "        for g_idx in range(batch_size):\n",
    "            indexs = torch.nonzero(graph2.batch == g_idx).squeeze(1)\n",
    "            g2_x_pooled[g_idx] = torch.sum(g2_x[indexs], 0)\n",
    "            \n",
    "        g3_x = F.relu(self.g3_gconv(g3_x, g3_edge_index)) # shape: (all node_number) * node_dimension\n",
    "        g3_x_pooled = torch.zeros((batch_size, self.node_dim)).to(device) # shape: batch_size * node_dimension\n",
    "        for g_idx in range(batch_size):\n",
    "            indexs = torch.nonzero(graph3.batch == g_idx).squeeze(1)\n",
    "            g3_x_pooled[g_idx] = torch.sum(g3_x[indexs], 0)\n",
    "        \n",
    "        hx = torch.randn((batch_size, int(self.node_dim/2) ), dtype=torch.float).to(device)\n",
    "        hx = self.rnn(g1_x_pooled, hx)\n",
    "        hx = self.rnn(g2_x_pooled, hx)\n",
    "        hx = self.rnn(g3_x_pooled, hx)\n",
    "\n",
    "        x = F.relu(self.linear1( torch.cat((hx, numeric_features), 1) ))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        y = F.relu(self.linear3(x))\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_at_k(predicted, labels, k = 10):\n",
    "    \n",
    "    # check whether both sizes are identical\n",
    "    assert predicted.size(0) == labels.size(0)\n",
    "    \n",
    "    # sort the values in descending order and gets the indexs\n",
    "    sorted_predicted_index = torch.argsort(predicted, descending = True)\n",
    "    sorted_label_index = torch.argsort(labels, descending = True)\n",
    "    \n",
    "    k_number = max(int(predicted.size(0) * k / 100), 1)\n",
    "    \n",
    "    topk_predicted_index = sorted_predicted_index[:k_number]\n",
    "    topk_label_index = sorted_label_index[:k_number]\n",
    "    \n",
    "    hit_count = 0\n",
    "    for p in topk_predicted_index:\n",
    "        if p in topk_label_index:\n",
    "            hit_count += 1\n",
    "            \n",
    "    accuracy = hit_count/k_number\n",
    "            \n",
    "    return (accuracy, hit_count, k_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Automatic Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_REGRESSION = 5e-3\n",
    "EPOCH = 10\n",
    "EARLY_STOP_PATIENCE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(lr_regression=LR_REGRESSION, max_epoch=EPOCH, early_stop_patience=EARLY_STOP_PATIENCE, verbose=True, manual_seed=None):\n",
    "    \n",
    "    if manual_seed:\n",
    "        seed = manual_seed\n",
    "    else:\n",
    "        seed = torch.random.seed()\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    popularity_model = PopularityModel(node_dimension = 8).to(device)\n",
    "    popularity_model.train()\n",
    "\n",
    "    # optimizer instances\n",
    "    optimizer_regression = torch.optim.Adam(popularity_model.parameters(), lr=lr_regression)\n",
    "    \n",
    "    train_losses = []\n",
    "\n",
    "    # cross validation for early stopping\n",
    "    current_val_error = float('inf')\n",
    "    val_error_inc_count = 0\n",
    "    cv_losses = []\n",
    "    \n",
    "    for epoch in range(max_epoch):\n",
    "    \n",
    "        if verbose:\n",
    "            print(f\"{epoch}\", end=\".\")\n",
    "\n",
    "        batch_losses = []\n",
    "        \n",
    "        train_h1_it = iter(train_h1_dataset)\n",
    "        train_h2_it = iter(train_h2_dataset)\n",
    "        train_h3_it = iter(train_h3_dataset)\n",
    "\n",
    "        # training \n",
    "        for i in range(0, train_labels.size(0), BATCH_SIZE):\n",
    "            \n",
    "            if verbose:\n",
    "                print(\".\", end=\"\")\n",
    "\n",
    "            optimizer_regression.zero_grad()\n",
    "            \n",
    "            END = (i + BATCH_SIZE) if (i + BATCH_SIZE) < train_labels.size(0) else train_labels.size(0)\n",
    "            \n",
    "            graph_h1, graph_h2, graph_h3 = next(train_h1_it), next(train_h2_it), next(train_h3_it)\n",
    "            graph_h1.to(device)\n",
    "            graph_h2.to(device)\n",
    "            graph_h3.to(device)\n",
    "            batch_numeric_features = train_numeric_tensors[i:END]\n",
    "            batch_labels = train_labels[i:END]\n",
    "\n",
    "            # forward: GNN->RNN->LR\n",
    "            predicted = popularity_model(graph_h1, graph_h2, graph_h3, batch_numeric_features)\n",
    "\n",
    "            # compute loss (weighted mean squared error)\n",
    "            loss = F.mse_loss(predicted, batch_labels, reduction='mean')\n",
    "            # loss = WeightedMSELoss(predicted, batch_labels) # bigger penalty on bigger cascade\n",
    "\n",
    "            # backward propagation\n",
    "            loss.backward()\n",
    "            optimizer_regression.step()\n",
    "\n",
    "            batch_losses.append(loss)\n",
    "\n",
    "        train_loss = torch.tensor(batch_losses).mean().item()\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Loss: {train_loss:.4f}\", end=\",\\n\")\n",
    "        \n",
    "        if (epoch > 0) and (train_loss == train_losses[-1]):\n",
    "            if verbose:\n",
    "                print(f\"early stopping triggered! stopped at epoch {epoch}\")\n",
    "                break\n",
    "                \n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # cross validation & early stopping\n",
    "        with torch.no_grad():\n",
    "\n",
    "            batch_losses = []\n",
    "            \n",
    "            cv_h1_it = iter(cv_h1_dataset)\n",
    "            cv_h2_it = iter(cv_h2_dataset)\n",
    "            cv_h3_it = iter(cv_h3_dataset)\n",
    "            \n",
    "            for i in range(0, cv_labels.size(0), BATCH_SIZE):\n",
    "                \n",
    "                END = (i + BATCH_SIZE) if (i + BATCH_SIZE) < cv_labels.size(0) else cv_labels.size(0)\n",
    "                \n",
    "                graph_h1, graph_h2, graph_h3 = next(cv_h1_it), next(cv_h2_it), next(cv_h3_it)\n",
    "                graph_h1.to(device)\n",
    "                graph_h2.to(device)\n",
    "                graph_h3.to(device)\n",
    "                batch_numeric_features = cv_numeric_tensors[i:END]\n",
    "                batch_labels = cv_labels[i:END]\n",
    "\n",
    "                # forward: GNN->RNN->LR\n",
    "                predicted = popularity_model(graph_h1, graph_h2, graph_h3, batch_numeric_features)\n",
    "\n",
    "                # compute loss (weighted mean squared error)\n",
    "                loss = F.mse_loss(predicted, batch_labels, reduction='mean')\n",
    "\n",
    "                batch_losses.append(loss)\n",
    "\n",
    "            cv_error = torch.tensor(batch_losses).mean().item()\n",
    "            cv_losses.append(cv_error)\n",
    "\n",
    "            if cv_error >= current_val_error:\n",
    "                val_error_inc_count += 1\n",
    "                current_val_error = cv_error\n",
    "                if val_error_inc_count >= early_stop_patience:\n",
    "                    if verbose:\n",
    "                        print(f\"early stopping triggered! stopped at epoch {epoch}\")\n",
    "                    break\n",
    "            else:\n",
    "                val_error_inc_count = 0\n",
    "                current_val_error = cv_error\n",
    "                \n",
    "    with torch.no_grad():\n",
    "\n",
    "        model_test_predicted = torch.zeros((test_labels.size(0),), dtype=torch.float).to(device)\n",
    "        \n",
    "        test_h1_it = iter(test_h1_dataset)\n",
    "        test_h2_it = iter(test_h2_dataset)\n",
    "        test_h3_it = iter(test_h3_dataset)\n",
    "        for i in range(0, test_labels.size(0), BATCH_SIZE):\n",
    "\n",
    "            graph_h1, graph_h2, graph_h3 = next(test_h1_it), next(test_h2_it), next(test_h3_it)\n",
    "            graph_h1.to(device)\n",
    "            graph_h2.to(device)\n",
    "            graph_h3.to(device)\n",
    "            \n",
    "            END = (i + BATCH_SIZE) if (i + BATCH_SIZE) < cv_labels.size(0) else cv_labels.size(0)\n",
    "            batch_numeric_features = test_numeric_tensors[i:END]\n",
    "\n",
    "            # forward: GNN->RNN->LR\n",
    "            model_test_predicted[i:END] = popularity_model(graph_h1, graph_h2, graph_h3, batch_numeric_features).squeeze(1)\n",
    "\n",
    "        testset_size = test_labels.size(0)\n",
    "\n",
    "        model_mae_scores = F.l1_loss(model_test_predicted, test_labels)\n",
    "        model_mse_scores = F.mse_loss(model_test_predicted, test_labels)\n",
    "\n",
    "        hit_rate_top1p = accuracy_at_k(model_test_predicted, test_labels.squeeze(1), 1)\n",
    "        hit_rate_top5p = accuracy_at_k(model_test_predicted, test_labels.squeeze(1), 5)\n",
    "        hit_rate_top10p = accuracy_at_k(model_test_predicted, test_labels.squeeze(1), 10)\n",
    "        hit_rate_top15p = accuracy_at_k(model_test_predicted, test_labels.squeeze(1), 15)\n",
    "\n",
    "        ndcg_score_1p = sklearn.metrics.ndcg_score(test_labels.reshape((1, -1)).cpu(), model_test_predicted.unsqueeze(0).cpu(), k=int(testset_size * 1 / 100))\n",
    "        ndcg_score_5p = sklearn.metrics.ndcg_score(test_labels.reshape((1, -1)).cpu(), model_test_predicted.unsqueeze(0).cpu(), k=int(testset_size * 5 / 100))\n",
    "        ndcg_score_10p = sklearn.metrics.ndcg_score(test_labels.reshape((1, -1)).cpu(), model_test_predicted.unsqueeze(0).cpu(), k=int(testset_size * 10 / 100))\n",
    "        ndcg_score_15p = sklearn.metrics.ndcg_score(test_labels.reshape((1, -1)).cpu(), model_test_predicted.unsqueeze(0).cpu(), k=int(testset_size * 15 / 100))\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\nseed: {seed}\")\n",
    "            print(f\"MAE: {model_mae_scores.item()}\")\n",
    "            print(f\"MSE: {model_mse_scores.item()}\")\n",
    "            print(f\"Hit Rate@1%: {hit_rate_top1p}\")\n",
    "            print(f\"Hit Rate@5%: {hit_rate_top5p}\")\n",
    "            print(f\"Hit Rate@10%: {hit_rate_top10p}\")\n",
    "            print(f\"Hit Rate@15%: {hit_rate_top15p}\")\n",
    "            print(f\"NDCG@1%: {ndcg_score_1p}\")\n",
    "            print(f\"NDCG@5%: {ndcg_score_5p}\")\n",
    "            print(f\"NDCG@10%: {ndcg_score_10p}\")\n",
    "            print(f\"NDCG@15%: {ndcg_score_15p}\")\n",
    "            \n",
    "            # plot loss curve\n",
    "            plt.plot(train_losses, label = 'training')\n",
    "            plt.plot(cv_losses, label = 'validation')\n",
    "            plt.xlabel('epoch'), plt.ylabel('MSE')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "        # clear useless CUDA memory\n",
    "        popularity_model = None\n",
    "        optimizer_regression = None\n",
    "        graph_h1, graph_h2, graph_h3 = None, None, None\n",
    "        batch_labels = None\n",
    "        predicted = None\n",
    "        loss = None\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return {\n",
    "            'seed': seed,\n",
    "            'mae': model_mae_scores.item(),\n",
    "            'mse': model_mse_scores.item(),\n",
    "            'hr1p': hit_rate_top1p[0],\n",
    "            'hr5p': hit_rate_top5p[0],\n",
    "            'hr10p': hit_rate_top10p[0],\n",
    "            'hr15p': hit_rate_top15p[0],\n",
    "            'ndcg1p': ndcg_score_1p,\n",
    "            'ndcg5p': ndcg_score_5p,\n",
    "            'ndcg10p': ndcg_score_10p,\n",
    "            'ndcg15p': ndcg_score_15p,\n",
    "            'param': {\n",
    "                'lr_regression': lr_regression,\n",
    "                'max_epoch': max_epoch,\n",
    "                'early_stop_patience': early_stop_patience\n",
    "            }\n",
    "            \n",
    "        };"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(lr_regression=2e-3, max_epoch=500, early_stop_patience=EARLY_STOP_PATIENCE, verbose=True, manual_seed=17643402734797685269)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_PER_SETTING = 10\n",
    "results = []\n",
    "for i in range(MODELS_PER_SETTING):\n",
    "    print(f\"model {i+1} - {datetime.now()}\")\n",
    "\n",
    "    res = train(lr_regression=2e-3, max_epoch=500, early_stop_patience=EARLY_STOP_PATIENCE, verbose=True, manual_seed=None)\n",
    "    res['number'] = i\n",
    "    res['score'] = res['hr1p'] + res['hr5p'] + res['hr10p'] + res['hr15p'] + res['ndcg1p'] + res['ndcg5p'] + res['ndcg10p'] + res['ndcg15p']\n",
    "    results.append(res)\n",
    "    print(f\"model {i+1} done - {datetime.now()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
