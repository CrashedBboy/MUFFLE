{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. TODO\n",
    "\n",
    "* [x] Evaluation metrics for ranking: accuracy@k, NDCG, Kendall's tau\n",
    "* [x] try new loss function (great loss on bigger cascade)\n",
    "* [x] BERT fine-tune\n",
    "* [x] Add other post's features: text entity count\n",
    "* [x] Make train / validation / test splitting fixed\n",
    "* [x] Check labels distribution for top-k cascades (because HR is poor)\n",
    "* [x] Do feature normalization before feeding into MLP\n",
    "* [ ] Up-sample inbalanced data\n",
    "* [x] Text data preprocessing\n",
    "* [x] Add temporal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers as ppb\n",
    "from transformers import AdamW\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "import os.path as path\n",
    "\n",
    "import sklearn\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print messages\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# device for training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device type: {device.type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NEWS = \"./data/all/news.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.read_csv(DATASET_NEWS, header = 0)\n",
    "print(f\"Columns: {news_df.columns}\")\n",
    "print(f\"news set size: {len(news_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_TRAIN = \"./data/splitted/source_tweet_processed_train.csv\"\n",
    "DATASET_CV = \"./data/splitted/source_tweet_processed_cv.csv\"\n",
    "DATASET_TEST = \"./data/splitted/source_tweet_processed_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATASET_TRAIN, header = 0)\n",
    "cv_df = pd.read_csv(DATASET_CV, header = 0)\n",
    "test_df = pd.read_csv(DATASET_TEST, header = 0)\n",
    "\n",
    "print(f\"Columns: {train_df.columns}\")\n",
    "print(f\"train set size: {len(train_df)}\")\n",
    "print(f\"cross validation set size: {len(cv_df)}\")\n",
    "print(f\"test set size: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for label distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_counts = train_df['cascade_size'].value_counts()\n",
    "cv_label_counts = cv_df['cascade_size'].value_counts()\n",
    "test_label_counts = test_df['cascade_size'].value_counts()\n",
    "\n",
    "print(f\"train set: label=1: {train_label_counts[1]/len(train_df):.4f}, label=2: {train_label_counts[2]/len(train_df):.4f}, label=3: {train_label_counts[3]/len(train_df):.4f}, label=4: {train_label_counts[4]/len(train_df):.4f}\")\n",
    "print(f\"cv set: label=1: {cv_label_counts[1]/len(cv_df):.4f}, label=2: {cv_label_counts[2]/len(cv_df):.4f}, label=3: {cv_label_counts[3]/len(cv_df):.4f}, label=4: {cv_label_counts[4]/len(cv_df):.4f}\")\n",
    "print(f\"test set: label=1: {test_label_counts[1]/len(test_df):.4f}, label=2: {test_label_counts[2]/len(test_df):.4f}, label=3: {test_label_counts[3]/len(test_df):.4f}, label=4: {test_label_counts[4]/len(test_df):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News Text Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained tokenizer\n",
    "\n",
    "# For DistilBERT:\n",
    "tokenizer = ppb.DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_title_tokenized = news_df['title'].apply(lambda x: tokenizer.encode(x, add_special_tokens = True))\n",
    "print(f\"news title tokenized shape: {news_title_tokenized.shape}\")\n",
    "\n",
    "news_text_tokenized = news_df['text'].apply(lambda x: tokenizer.encode(x, add_special_tokens = True))\n",
    "print(f\"news test tokenized shape: {news_text_tokenized.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "padding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_length = 0\n",
    "\n",
    "for i in news_title_tokenized.values:\n",
    "    total_length += len(i)\n",
    "    \n",
    "avg_news_title_length = int(total_length / news_title_tokenized.shape[0])\n",
    "print(avg_news_title_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_length = 0\n",
    "\n",
    "for i in news_text_tokenized.values:\n",
    "    total_length += len(i)\n",
    "    \n",
    "avg_news_text_length = int(total_length / news_text_tokenized.shape[0])\n",
    "print(avg_news_text_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWS_TITLE_LENGTH = 20\n",
    "NEWS_TEXT_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate news title and text \n",
    "for i in range(len(news_title_tokenized)):\n",
    "    news_title_tokenized[i] = news_title_tokenized[i][:NEWS_TITLE_LENGTH]\n",
    "for i in range(len(news_text_tokenized)):\n",
    "    news_text_tokenized[i] = news_text_tokenized[i][:NEWS_TEXT_LENGTH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_title_padded = np.array([i + [0] * (NEWS_TITLE_LENGTH - len(i)) for i in news_title_tokenized.values])\n",
    "news_text_padded = np.array([i + [0] * (NEWS_TEXT_LENGTH - len(i)) for i in news_text_tokenized.values])\n",
    "\n",
    "print(f\"news_title_padded: {news_title_padded.shape}\")\n",
    "print(f\"news_text_padded: {news_text_padded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "masking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_title_attention_mask = np.where(news_title_padded != 0, 1, 0)\n",
    "news_text_attention_mask = np.where(news_text_padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put it into GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWS_NUMERIC_COLUMNS = ['img_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news_text_tensors = []\n",
    "cv_news_text_tensors = []\n",
    "test_news_text_tensors = []\n",
    "train_news_text_mask = []\n",
    "cv_news_text_mask = []\n",
    "test_news_text_mask = []\n",
    "\n",
    "train_news_title_tensors = []\n",
    "cv_news_title_tensors = []\n",
    "test_news_title_tensors = []\n",
    "train_news_title_mask = []\n",
    "cv_news_title_mask = []\n",
    "test_news_title_mask = []\n",
    "\n",
    "train_news_numeric_tensors = []\n",
    "cv_news_numeric_tensors = []\n",
    "test_news_numeric_tensors = []\n",
    "\n",
    "for idx, row in train_df.iterrows():\n",
    "    news_id = row['news_id']\n",
    "    news_row_id = news_df[news_df['id'] == news_id].index[0]\n",
    "    \n",
    "    train_news_text_tensors.append(news_text_padded[news_row_id])\n",
    "    train_news_text_mask.append(news_text_padded[news_row_id])\n",
    "    train_news_title_tensors.append(news_title_padded[news_row_id])\n",
    "    train_news_title_mask.append(news_title_padded[news_row_id])\n",
    "    \n",
    "    train_news_numeric_tensors.append(news_df[NEWS_NUMERIC_COLUMNS].iloc[news_row_id].values)\n",
    "    \n",
    "for idx, row in cv_df.iterrows():\n",
    "    news_id = row['news_id']\n",
    "    news_row_id = news_df[news_df['id'] == news_id].index[0]\n",
    "    \n",
    "    cv_news_text_tensors.append(news_text_padded[news_row_id])\n",
    "    cv_news_text_mask.append(news_text_padded[news_row_id])\n",
    "    cv_news_title_tensors.append(news_title_padded[news_row_id])\n",
    "    cv_news_title_mask.append(news_title_padded[news_row_id])\n",
    "    \n",
    "    cv_news_numeric_tensors.append(news_df[NEWS_NUMERIC_COLUMNS].iloc[news_row_id].values)\n",
    "    \n",
    "for idx, row in test_df.iterrows():\n",
    "    news_id = row['news_id']\n",
    "    news_row_id = news_df[news_df['id'] == news_id].index[0]\n",
    "    \n",
    "    test_news_text_tensors.append(news_text_padded[news_row_id])\n",
    "    test_news_text_mask.append(news_text_padded[news_row_id])\n",
    "    test_news_title_tensors.append(news_title_padded[news_row_id])\n",
    "    test_news_title_mask.append(news_title_padded[news_row_id])\n",
    "    \n",
    "    test_news_numeric_tensors.append(news_df[NEWS_NUMERIC_COLUMNS].iloc[news_row_id].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news_text_tensors = torch.tensor(train_news_text_tensors, dtype=torch.long).to(device)\n",
    "cv_news_text_tensors = torch.tensor(cv_news_text_tensors, dtype=torch.long).to(device)\n",
    "test_news_text_tensors = torch.tensor(test_news_text_tensors, dtype=torch.long).to(device)\n",
    "train_news_text_mask = torch.tensor(train_news_text_mask, dtype=torch.long).to(device)\n",
    "cv_news_text_mask = torch.tensor(cv_news_text_mask, dtype=torch.long).to(device)\n",
    "test_news_text_mask = torch.tensor(test_news_text_mask, dtype=torch.long).to(device)\n",
    "\n",
    "train_news_title_tensors = torch.tensor(train_news_title_tensors, dtype=torch.long).to(device)\n",
    "cv_news_title_tensors = torch.tensor(cv_news_title_tensors, dtype=torch.long).to(device)\n",
    "test_news_title_tensors = torch.tensor(test_news_title_tensors, dtype=torch.long).to(device)\n",
    "train_news_title_mask = torch.tensor(train_news_title_mask, dtype=torch.long).to(device)\n",
    "cv_news_title_mask = torch.tensor(cv_news_title_mask, dtype=torch.long).to(device)\n",
    "test_news_title_mask = torch.tensor(test_news_title_mask, dtype=torch.long).to(device)\n",
    "\n",
    "train_news_numeric_tensors = torch.tensor(train_news_numeric_tensors, dtype=torch.long).to(device)\n",
    "cv_news_numeric_tensors = torch.tensor(cv_news_numeric_tensors, dtype=torch.long).to(device)\n",
    "test_news_numeric_tensors = torch.tensor(test_news_numeric_tensors, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Text Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = train_df['text'].apply(lambda x: tokenizer.encode(x, add_special_tokens = True))\n",
    "cv_tokenized = cv_df['text'].apply(lambda x: tokenizer.encode(x, add_special_tokens = True))\n",
    "test_tokenized = test_df['text'].apply(lambda x: tokenizer.encode(x, add_special_tokens = True))\n",
    "\n",
    "print(f\"train tokenized shape: {train_tokenized.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_length = 0\n",
    "\n",
    "for i in train_tokenized.values:\n",
    "    total_length += len(i)\n",
    "for i in cv_tokenized.values:\n",
    "    total_length += len(i)\n",
    "for i in test_tokenized.values:\n",
    "    total_length += len(i)\n",
    "    \n",
    "average_length = int(total_length / (train_tokenized.shape[0] + cv_tokenized.shape[0] + test_tokenized.shape[0]))\n",
    "print(average_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "for i in train_tokenized.values:\n",
    "    lengths.append(len(i))\n",
    "for i in cv_tokenized.values:\n",
    "    lengths.append(len(i))\n",
    "for i in test_tokenized.values:\n",
    "    lengths.append(len(i))\n",
    "    \n",
    "median_length = np.median(np.array(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find max length among all sentences\n",
    "\n",
    "max_len = 0\n",
    "for i in train_tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "for i in cv_tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "for i in test_tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "        \n",
    "print(f\"Max sentence length: {max_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we set a fixed text length, which is between max sentence length and average length\n",
    "TEXT_LENGTH = int(median_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncate description using average length\n",
    "for i in range(len(train_tokenized)):\n",
    "    train_tokenized[i] = train_tokenized[i][:TEXT_LENGTH]\n",
    "for i in range(len(cv_tokenized)):\n",
    "    cv_tokenized[i] = cv_tokenized[i][:TEXT_LENGTH]\n",
    "for i in range(len(test_tokenized)):\n",
    "    test_tokenized[i] = test_tokenized[i][:TEXT_LENGTH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded = np.array([i + [0] * (TEXT_LENGTH - len(i)) for i in train_tokenized.values])\n",
    "cv_padded = np.array([i + [0] * (TEXT_LENGTH - len(i)) for i in cv_tokenized.values])\n",
    "test_padded = np.array([i + [0] * (TEXT_LENGTH - len(i)) for i in test_tokenized.values])\n",
    "\n",
    "print(f\"train_padded: {train_padded.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Masking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_attention_mask = np.where(train_padded != 0, 1, 0)\n",
    "cv_attention_mask = np.where(cv_padded != 0, 1, 0)\n",
    "test_attention_mask = np.where(test_padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put into GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_tensors = torch.tensor(train_padded, dtype=torch.long).to(device)\n",
    "cv_text_tensors = torch.tensor(cv_padded, dtype=torch.long).to(device)\n",
    "test_text_tensors = torch.tensor(test_padded, dtype=torch.long).to(device)\n",
    "\n",
    "train_text_mask = torch.tensor(train_attention_mask, dtype=torch.long).to(device)\n",
    "cv_text_mask = torch.tensor(cv_attention_mask, dtype=torch.long).to(device)\n",
    "test_text_mask = torch.tensor(test_attention_mask, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numeric Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistic_columns = ['user_count', 'tag_count', 'symbol_count', 'url_count', 'sentence_count']\n",
    "\n",
    "wds = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n",
    "temporal_columns = [f\"h_{h:02}\" for h in range(0,24)] + [f\"wday_{wd}\" for wd in wds] + ['is_holiday']\n",
    "\n",
    "sentiment_columns = ['avg_sentiment_score', 'sentiment_ratio', 'pos_count', 'neg_count']\n",
    "\n",
    "numeric_columns = statistic_columns + temporal_columns + sentiment_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_numeric_tensors = torch.tensor(train_df[numeric_columns].values, dtype=torch.float).to(device)\n",
    "cv_numeric_tensors = torch.tensor(cv_df[numeric_columns].values, dtype=torch.float).to(device)\n",
    "test_numeric_tensors = torch.tensor(test_df[numeric_columns].values, dtype=torch.float).to(device)\n",
    "\n",
    "print(train_numeric_tensors.shape)\n",
    "print(cv_numeric_tensors.shape)\n",
    "print(test_numeric_tensors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = torch.tensor(train_df['cascade_size'].values, dtype=torch.float).unsqueeze(1).to(device)\n",
    "cv_labels = torch.tensor(cv_df['cascade_size'].values, dtype=torch.float).unsqueeze(1).to(device)\n",
    "test_labels = torch.tensor(test_df['cascade_size'].values, dtype=torch.float).unsqueeze(1).to(device)\n",
    "\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling the minor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shifted = torch.clone(train_labels).squeeze(1)\n",
    "# shifted -= 1\n",
    "# nonzero_indexs = torch.nonzero(shifted).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# up_train_text_tensors = torch.clone(train_text_tensors)[nonzero_indexs]\n",
    "# up_train_text_mask = torch.clone(train_text_mask)[nonzero_indexs]\n",
    "# up_train_statistic_tensors =  torch.clone(train_statistic_tensors)[nonzero_indexs]\n",
    "# up_train_temporal_tensors = torch.clone(train_temporal_tensors)[nonzero_indexs]\n",
    "# up_train_labels = torch.clone(train_labels)[nonzero_indexs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted MSE Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WeightedMSELoss(predicted, labels):\n",
    "    return torch.sum(F.mse_loss(predicted, labels, reduction='none') * labels) / predicted.size(0) # bigger penalty on bigger cascade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training: BERT (with fine-tuning) + MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PopularityModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, tweet_in_dim=768, tweet_to_dim=128,\n",
    "                 news_title_in_dim=768, news_title_to_dim=64,\n",
    "                 news_text_in_dim=768, news_text_to_dim=128,\n",
    "                 other_dimension = 42):\n",
    "        super(PopularityModel, self).__init__()\n",
    "\n",
    "        self.MLP_INPUT_DIM = tweet_to_dim + news_title_to_dim + news_text_to_dim + other_dimension\n",
    "        \n",
    "        self.tweet_linear = nn.Linear(tweet_in_dim, tweet_to_dim)\n",
    "        self.news_title_linear = nn.Linear(news_title_in_dim, news_title_to_dim)\n",
    "        self.news_text_linear = nn.Linear(news_text_in_dim, news_text_to_dim)\n",
    "        \n",
    "        self.linear1 = nn.Linear(self.MLP_INPUT_DIM, int(self.MLP_INPUT_DIM/2))\n",
    "        self.linear2 = nn.Linear(int(self.MLP_INPUT_DIM/2), int(self.MLP_INPUT_DIM/4))\n",
    "        self.linear3 = nn.Linear(int(self.MLP_INPUT_DIM/4), int(self.MLP_INPUT_DIM/8))\n",
    "        self.linear4 = nn.Linear(int(self.MLP_INPUT_DIM/8), 1)\n",
    "        \n",
    "    def forward(self, tweet_embedded, news_title_embedded, news_text_embedded, other_features):\n",
    "        \n",
    "        tweet_downsampled = F.relu(self.tweet_linear(tweet_embedded))\n",
    "        news_title_downsampled = F.relu(self.news_title_linear(news_title_embedded))\n",
    "        news_text_downsampled = F.relu(self.news_text_linear(news_text_embedded))\n",
    "        \n",
    "        # text concatenated with statistical features, temporal features\n",
    "        y = torch.cat((tweet_downsampled, news_title_downsampled, news_text_downsampled, other_features), 1)\n",
    "        \n",
    "        # MLP\n",
    "        y = F.relu(self.linear1(y))\n",
    "        y = F.relu(self.linear2(y))\n",
    "        y = F.relu(self.linear3(y))\n",
    "        y = F.relu(self.linear4(y))\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check label distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_test_label_indexs = torch.argsort(test_labels.squeeze(0), dim=0, descending = True).squeeze(1)\n",
    "\n",
    "test_labels[sorted_test_label_indexs[:int(test_labels.size(0) * 0.15)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics for Ranking\n",
    "related article: [Evaluation Metrics for Ranking problems: Introduction and Examples](https://queirozf.com/entries/evaluation-metrics-for-ranking-problems-introduction-and-examples)\n",
    "\n",
    "* Accuracy@K (Hit Rate)\n",
    "* NDCG\n",
    "\n",
    "`note`: Here we define K = 1%, 5%, 10%, 15%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy@K(Hit Rate):**  \n",
    "\n",
    "Accuracy@K  \n",
    "= precision@K  \n",
    "= true positives@K / (true positives@K + false positives@K)  \n",
    "= recall@K  \n",
    "= true positives@K / (true positives@K + false negative@K)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_at_k(predicted, labels, k = 10):\n",
    "    \n",
    "    # check whether both sizes are identical\n",
    "    assert predicted.size(0) == labels.size(0)\n",
    "    \n",
    "    # sort the values in descending order and gets the indexs\n",
    "    sorted_predicted_index = torch.argsort(predicted, descending = True)\n",
    "    sorted_label_index = torch.argsort(labels, descending = True)\n",
    "    \n",
    "    k_number = max(int(predicted.size(0) * k / 100), 1)\n",
    "    \n",
    "    topk_predicted_index = sorted_predicted_index[:k_number]\n",
    "    topk_label_index = sorted_label_index[:k_number]\n",
    "    \n",
    "    hit_count = 0\n",
    "    for p in topk_predicted_index:\n",
    "        if p in topk_label_index:\n",
    "            hit_count += 1\n",
    "            \n",
    "    accuracy = hit_count/k_number\n",
    "            \n",
    "    return (accuracy, hit_count, k_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 。Candidates of Comparison\n",
    "\n",
    "1. our model (fitted)\n",
    "2. historical average\n",
    "3. random - our model without fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 。HA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ha_predicted = torch.ones( (test_labels.shape[0],) ) * train_labels.mean().item()\n",
    "ha_predicted = ha_predicted.to(device)\n",
    "\n",
    "testset_size = test_labels.size(0)\n",
    "\n",
    "model_mae_scores = F.l1_loss(ha_predicted, test_labels)\n",
    "model_mse_scores = F.mse_loss(ha_predicted, test_labels)\n",
    "\n",
    "hit_rate_top1p = accuracy_at_k(ha_predicted, test_labels, 1)\n",
    "hit_rate_top5p = accuracy_at_k(ha_predicted, test_labels, 5)\n",
    "hit_rate_top10p = accuracy_at_k(ha_predicted, test_labels, 10)\n",
    "hit_rate_top15p = accuracy_at_k(ha_predicted, test_labels, 15)\n",
    "\n",
    "ndcg_score_1p = sklearn.metrics.ndcg_score(test_labels.reshape((1, -1)).cpu(), ha_predicted.unsqueeze(0).cpu(), k=int(testset_size * 1 / 100))\n",
    "ndcg_score_5p = sklearn.metrics.ndcg_score(test_labels.reshape((1, -1)).cpu(), ha_predicted.unsqueeze(0).cpu(), k=int(testset_size * 5 / 100))\n",
    "ndcg_score_10p = sklearn.metrics.ndcg_score(test_labels.reshape((1, -1)).cpu(), ha_predicted.unsqueeze(0).cpu(), k=int(testset_size * 10 / 100))\n",
    "ndcg_score_15p = sklearn.metrics.ndcg_score(test_labels.reshape((1, -1)).cpu(), ha_predicted.unsqueeze(0).cpu(), k=int(testset_size * 15 / 100))\n",
    "\n",
    "print(f\"MAE: {model_mae_scores.item()}\")\n",
    "print(f\"MSE: {model_mse_scores.item()}\")\n",
    "print(f\"Hit Rate@1%: {hit_rate_top1p}\")\n",
    "print(f\"Hit Rate@5%: {hit_rate_top5p}\")\n",
    "print(f\"Hit Rate@10%: {hit_rate_top10p}\")\n",
    "print(f\"Hit Rate@15%: {hit_rate_top15p}\")\n",
    "print(f\"NDCG@1%: {ndcg_score_1p}\")\n",
    "print(f\"NDCG@5%: {ndcg_score_5p}\")\n",
    "print(f\"NDCG@10%: {ndcg_score_10p}\")\n",
    "print(f\"NDCG@15%: {ndcg_score_15p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 。HM (Historical Median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.median().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm_predicted = torch.ones( (test_labels.shape[0],) ) * train_labels.median().item()\n",
    "hm_predicted = hm_predicted.to(device)\n",
    "\n",
    "testset_size = test_labels.size(0)\n",
    "\n",
    "model_mae_scores = F.l1_loss(hm_predicted, test_labels)\n",
    "model_mse_scores = F.mse_loss(hm_predicted, test_labels)\n",
    "\n",
    "hit_rate_top1p = accuracy_at_k(hm_predicted, test_labels, 1)\n",
    "hit_rate_top5p = accuracy_at_k(hm_predicted, test_labels, 5)\n",
    "hit_rate_top10p = accuracy_at_k(hm_predicted, test_labels, 10)\n",
    "hit_rate_top15p = accuracy_at_k(hm_predicted, test_labels, 15)\n",
    "\n",
    "ndcg_score_1p = sklearn.metrics.ndcg_score(test_labels.reshape((1, -1)).cpu(), hm_predicted.unsqueeze(0).cpu(), k=int(testset_size * 1 / 100))\n",
    "ndcg_score_5p = sklearn.metrics.ndcg_score(test_labels.reshape((1, -1)).cpu(), hm_predicted.unsqueeze(0).cpu(), k=int(testset_size * 5 / 100))\n",
    "ndcg_score_10p = sklearn.metrics.ndcg_score(test_labels.reshape((1, -1)).cpu(), hm_predicted.unsqueeze(0).cpu(), k=int(testset_size * 10 / 100))\n",
    "ndcg_score_15p = sklearn.metrics.ndcg_score(test_labels.reshape((1, -1)).cpu(), hm_predicted.unsqueeze(0).cpu(), k=int(testset_size * 15 / 100))\n",
    "\n",
    "print(f\"MAE: {model_mae_scores.item()}\")\n",
    "print(f\"MSE: {model_mse_scores.item()}\")\n",
    "print(f\"Hit Rate@1%: {hit_rate_top1p}\")\n",
    "print(f\"Hit Rate@5%: {hit_rate_top5p}\")\n",
    "print(f\"Hit Rate@10%: {hit_rate_top10p}\")\n",
    "print(f\"Hit Rate@15%: {hit_rate_top15p}\")\n",
    "print(f\"NDCG@1%: {ndcg_score_1p}\")\n",
    "print(f\"NDCG@5%: {ndcg_score_5p}\")\n",
    "print(f\"NDCG@10%: {ndcg_score_10p}\")\n",
    "print(f\"NDCG@15%: {ndcg_score_15p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Automatic Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_REGRESSION = 0.2e-3 # learning rate for linear regression\n",
    "LR_BERT_TWEET = 1e-5 # learning rate for BERT (tweet)\n",
    "LR_BERT_NEWS_TITLE = 1e-6 # learning rate for BERT (title)\n",
    "LR_BERT_NEWS_TEXT = 1e-6 # learning rate for BERT (text)\n",
    "EPOCH = 500\n",
    "BATCH_SIZE = 720\n",
    "EARLY_STOP_PATIENCE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(regression_lr=LR_REGRESSION, bert_tweet_lr=LR_BERT_TWEET, bert_news_title_lr=LR_BERT_NEWS_TITLE, bert_news_text_lr=LR_BERT_NEWS_TEXT,\n",
    "          max_epoch=EPOCH, batch_size=BATCH_SIZE, early_stop_patience=EARLY_STOP_PATIENCE, verbose=True, manual_seed=None):\n",
    "    \n",
    "    if manual_seed:\n",
    "        seed = manual_seed\n",
    "    else:\n",
    "        seed = torch.random.seed()\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    popularity_model = PopularityModel(tweet_in_dim=768, tweet_to_dim=128,\n",
    "                 news_title_in_dim=768, news_title_to_dim=64,\n",
    "                 news_text_in_dim=768, news_text_to_dim=128,\n",
    "                 other_dimension = 42).to(device)\n",
    "    popularity_model.train()\n",
    "    \n",
    "    # BERT for tweet\n",
    "    bert_model_tweet = ppb.DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
    "    # fine-tune only last layer of BERT\n",
    "    for param in bert_model_tweet.parameters():\n",
    "        param.requires_grad = False\n",
    "    for p in bert_model_tweet.transformer.layer[len(bert_model_tweet.transformer.layer) - 1].parameters():\n",
    "        p.requires_grad = True\n",
    "        \n",
    "    # BERT for news title\n",
    "    bert_model_news_title = ppb.DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
    "    # fine-tune only last layer of BERT\n",
    "    for param in bert_model_news_title.parameters():\n",
    "        param.requires_grad = False\n",
    "    for p in bert_model_news_title.transformer.layer[len(bert_model_news_title.transformer.layer) - 1].parameters():\n",
    "        p.requires_grad = True\n",
    "        \n",
    "    # BERT for news text\n",
    "    bert_model_news_text = ppb.DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
    "    # fine-tune only last layer of BERT\n",
    "    for param in bert_model_news_text.parameters():\n",
    "        param.requires_grad = False\n",
    "    for p in bert_model_news_text.transformer.layer[len(bert_model_news_text.transformer.layer) - 1].parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    optimizer_regression = torch.optim.Adam(popularity_model.parameters(), lr = regression_lr)\n",
    "    optimizer_bert_tweet = AdamW(bert_model_tweet.parameters(), lr=bert_tweet_lr)\n",
    "    optimizer_bert_news_title = AdamW(bert_model_news_title.parameters(), lr=bert_news_title_lr)\n",
    "    optimizer_bert_news_text = AdamW(bert_model_news_text.parameters(), lr=bert_news_text_lr)\n",
    "    \n",
    "    epoch_losses = []\n",
    "\n",
    "    # cross validation for early stopping\n",
    "    current_val_error = float('inf')\n",
    "    val_error_inc_count = 0\n",
    "    cv_losses = []\n",
    "    \n",
    "    for epoch in range(max_epoch):\n",
    "    \n",
    "        if verbose:\n",
    "            print(epoch, end=\",\")\n",
    "\n",
    "        batch_losses = []\n",
    "\n",
    "        # batch training\n",
    "        for i in range(0, train_labels.size(0), batch_size):\n",
    "\n",
    "            optimizer_regression.zero_grad()\n",
    "            optimizer_bert_tweet.zero_grad()\n",
    "            optimizer_bert_news_title.zero_grad()\n",
    "            optimizer_bert_news_text.zero_grad()\n",
    "\n",
    "            END = (i + batch_size) if (i + batch_size) < train_labels.size(0) else train_labels.size(0)\n",
    "        \n",
    "            batch_tweet_features = train_text_tensors[i:END]\n",
    "            batch_tweet_mask = train_text_mask[i:END]\n",
    "            batch_news_title_features = train_news_title_tensors[i:END]\n",
    "            batch_news_title_mask = train_news_title_mask[i:END]\n",
    "            batch_news_text_features = train_news_text_tensors[i:END]\n",
    "            batch_news_text_mask = train_news_text_mask[i:END]\n",
    "            batch_numeric_features = torch.cat((train_numeric_tensors[i:END], train_news_numeric_tensors[i:END]), 1)\n",
    "            batch_labels = train_labels[i:END]\n",
    "\n",
    "            # forward: BERT embedding for tweet\n",
    "            last_hidden_states = bert_model_tweet(batch_tweet_features, attention_mask=batch_tweet_mask)\n",
    "            # The embedded CLS token can be thought of as an embedding for the entire sentence!!\n",
    "            tweet_embedded = last_hidden_states[0][:, 0, :]\n",
    "            \n",
    "            # forward: BERT embedding for news title\n",
    "            last_hidden_states = bert_model_news_title(batch_news_title_features, attention_mask=batch_news_title_mask)\n",
    "            # The embedded CLS token can be thought of as an embedding for the entire sentence!!\n",
    "            news_title_embedded = last_hidden_states[0][:, 0, :]\n",
    "            \n",
    "            # forward: BERT embedding for tweet\n",
    "            last_hidden_states = bert_model_news_text(batch_news_text_features, attention_mask=batch_news_text_mask)\n",
    "            # The embedded CLS token can be thought of as an embedding for the entire sentence!!\n",
    "            news_text_embedded = last_hidden_states[0][:, 0, :]\n",
    "\n",
    "            # forward: Linear Regression\n",
    "            predicted = popularity_model(tweet_embedded, news_title_embedded, news_text_embedded, batch_numeric_features)\n",
    "\n",
    "            # compute loss (weighted mean squared error)\n",
    "            loss = F.mse_loss(predicted, batch_labels, reduction='mean')\n",
    "\n",
    "            # backward propagation\n",
    "            loss.backward()\n",
    "            optimizer_regression.step()\n",
    "            optimizer_bert_tweet.step()\n",
    "            optimizer_bert_news_title.step()\n",
    "            optimizer_bert_news_text.step()\n",
    "\n",
    "            batch_losses.append(loss)\n",
    "\n",
    "        epoch_losses.append(torch.tensor(batch_losses).mean().item())\n",
    "        \n",
    "        with torch.no_grad():\n",
    "        \n",
    "            batch_losses = []\n",
    "            for i in range(0, cv_labels.size(0), batch_size):\n",
    "\n",
    "                END = (i + batch_size) if (i + batch_size) < cv_labels.size(0) else cv_labels.size(0)\n",
    "                \n",
    "                batch_tweet_features = cv_text_tensors[i:END]\n",
    "                batch_tweet_mask = cv_text_mask[i:END]\n",
    "                batch_news_title_features = cv_news_title_tensors[i:END]\n",
    "                batch_news_title_mask = cv_news_title_mask[i:END]\n",
    "                batch_news_text_features = cv_news_text_tensors[i:END]\n",
    "                batch_news_text_mask = cv_news_text_mask[i:END]\n",
    "                batch_numeric_features = torch.cat((cv_numeric_tensors[i:END], cv_news_numeric_tensors[i:END]), 1)\n",
    "                batch_labels = cv_labels[i:END]\n",
    "\n",
    "                # forward: BERT embedding for tweet\n",
    "                last_hidden_states = bert_model_tweet(batch_tweet_features, attention_mask=batch_tweet_mask)\n",
    "                # The embedded CLS token can be thought of as an embedding for the entire sentence!!\n",
    "                tweet_embedded = last_hidden_states[0][:, 0, :]\n",
    "\n",
    "                # forward: BERT embedding for news title\n",
    "                last_hidden_states = bert_model_news_title(batch_news_title_features, attention_mask=batch_news_title_mask)\n",
    "                # The embedded CLS token can be thought of as an embedding for the entire sentence!!\n",
    "                news_title_embedded = last_hidden_states[0][:, 0, :]\n",
    "\n",
    "                # forward: BERT embedding for tweet\n",
    "                last_hidden_states = bert_model_news_text(batch_news_text_features, attention_mask=batch_news_text_mask)\n",
    "                # The embedded CLS token can be thought of as an embedding for the entire sentence!!\n",
    "                news_text_embedded = last_hidden_states[0][:, 0, :]\n",
    "\n",
    "                # forward: Linear Regression\n",
    "                predicted = popularity_model(tweet_embedded, news_title_embedded, news_text_embedded, batch_numeric_features)\n",
    "\n",
    "                # compute loss (weighted mean squared error)\n",
    "                loss = F.mse_loss(predicted, batch_labels, reduction='mean')\n",
    "\n",
    "                batch_losses.append(loss)\n",
    "\n",
    "            cv_error = torch.tensor(batch_losses).mean().item()\n",
    "            cv_losses.append(cv_error)\n",
    "\n",
    "            if cv_error >= current_val_error:\n",
    "                val_error_inc_count += 1\n",
    "                current_val_error = cv_error\n",
    "                if val_error_inc_count >= early_stop_patience:\n",
    "                    if verbose:\n",
    "                        print(f\"early stopping triggered! stopped at epoch {epoch}\")\n",
    "                    break\n",
    "            else:\n",
    "                val_error_inc_count = 0\n",
    "                current_val_error = cv_error\n",
    "    \n",
    "    # evaluation\n",
    "    with torch.no_grad():\n",
    "\n",
    "        model_test_predicted = torch.zeros((test_labels.size(0),), dtype=torch.float).to(device)\n",
    "        for i in range(0, test_labels.size(0), batch_size):\n",
    "\n",
    "            END = (i + batch_size) if (i + batch_size) < test_labels.size(0) else test_labels.size(0)\n",
    "\n",
    "            batch_tweet_features = test_text_tensors[i:END]\n",
    "            batch_tweet_mask = test_text_mask[i:END]\n",
    "            batch_news_title_features = test_news_title_tensors[i:END]\n",
    "            batch_news_title_mask = test_news_title_mask[i:END]\n",
    "            batch_news_text_features = test_news_text_tensors[i:END]\n",
    "            batch_news_text_mask = test_news_text_mask[i:END]\n",
    "            batch_numeric_features = torch.cat((test_numeric_tensors[i:END], test_news_numeric_tensors[i:END]), 1)\n",
    "            batch_labels = test_labels[i:END]\n",
    "\n",
    "            # forward: BERT embedding for tweet\n",
    "            last_hidden_states = bert_model_tweet(batch_tweet_features, attention_mask=batch_tweet_mask)\n",
    "            # The embedded CLS token can be thought of as an embedding for the entire sentence!!\n",
    "            tweet_embedded = last_hidden_states[0][:, 0, :]\n",
    "\n",
    "            # forward: BERT embedding for news title\n",
    "            last_hidden_states = bert_model_news_title(batch_news_title_features, attention_mask=batch_news_title_mask)\n",
    "            # The embedded CLS token can be thought of as an embedding for the entire sentence!!\n",
    "            news_title_embedded = last_hidden_states[0][:, 0, :]\n",
    "\n",
    "            # forward: BERT embedding for tweet\n",
    "            last_hidden_states = bert_model_news_text(batch_news_text_features, attention_mask=batch_news_text_mask)\n",
    "            # The embedded CLS token can be thought of as an embedding for the entire sentence!!\n",
    "            news_text_embedded = last_hidden_states[0][:, 0, :]\n",
    "\n",
    "            # forward: Linear Regression\n",
    "            model_test_predicted[i:END] = popularity_model(tweet_embedded, news_title_embedded, news_text_embedded, batch_numeric_features).squeeze(1)\n",
    "\n",
    "        testset_size = test_labels.size(0)\n",
    "\n",
    "        model_mae_scores = F.l1_loss(model_test_predicted, test_labels)\n",
    "        model_mse_scores = F.mse_loss(model_test_predicted, test_labels)\n",
    "\n",
    "        hit_rate_top1p = accuracy_at_k(model_test_predicted, test_labels.squeeze(1), 1)\n",
    "        hit_rate_top5p = accuracy_at_k(model_test_predicted, test_labels.squeeze(1), 5)\n",
    "        hit_rate_top10p = accuracy_at_k(model_test_predicted, test_labels.squeeze(1), 10)\n",
    "        hit_rate_top15p = accuracy_at_k(model_test_predicted, test_labels.squeeze(1), 15)\n",
    "\n",
    "        ndcg_score_1p = sklearn.metrics.ndcg_score(test_labels.reshape((1, -1)).cpu(), model_test_predicted.unsqueeze(0).cpu(), k=int(testset_size * 1 / 100))\n",
    "        ndcg_score_5p = sklearn.metrics.ndcg_score(test_labels.reshape((1, -1)).cpu(), model_test_predicted.unsqueeze(0).cpu(), k=int(testset_size * 5 / 100))\n",
    "        ndcg_score_10p = sklearn.metrics.ndcg_score(test_labels.reshape((1, -1)).cpu(), model_test_predicted.unsqueeze(0).cpu(), k=int(testset_size * 10 / 100))\n",
    "        ndcg_score_15p = sklearn.metrics.ndcg_score(test_labels.reshape((1, -1)).cpu(), model_test_predicted.unsqueeze(0).cpu(), k=int(testset_size * 15 / 100))\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"seed: {seed}\")\n",
    "            print(f\"MAE: {model_mae_scores.item()}\")\n",
    "            print(f\"MSE: {model_mse_scores.item()}\")\n",
    "            print(f\"Hit Rate@1%: {hit_rate_top1p}\")\n",
    "            print(f\"Hit Rate@5%: {hit_rate_top5p}\")\n",
    "            print(f\"Hit Rate@10%: {hit_rate_top10p}\")\n",
    "            print(f\"Hit Rate@15%: {hit_rate_top15p}\")\n",
    "            print(f\"NDCG@1%: {ndcg_score_1p}\")\n",
    "            print(f\"NDCG@5%: {ndcg_score_5p}\")\n",
    "            print(f\"NDCG@10%: {ndcg_score_10p}\")\n",
    "            print(f\"NDCG@15%: {ndcg_score_15p}\")\n",
    "            \n",
    "            plt.plot(epoch_losses, label = 'training')\n",
    "            plt.plot(cv_losses, label = 'validation')\n",
    "            plt.xlabel('epoch'), plt.ylabel('MSE')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "        # clear useless CUDA memory\n",
    "        popularity_model = None\n",
    "        bert_model = None\n",
    "        optimizer_regression = None\n",
    "        optimizer_bert = None\n",
    "        batch_text_features = None\n",
    "        batch_text_mask = None\n",
    "        batch_statistic_features = None\n",
    "        batch_temporal_features = None\n",
    "        batch_labels = None\n",
    "        last_hidden_states = None\n",
    "        embedded = None\n",
    "        predicted = None\n",
    "        loss = None\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return {\n",
    "            'seed': seed,\n",
    "            'mae': model_mae_scores.item(),\n",
    "            'mse': model_mse_scores.item(),\n",
    "            'hr1p': hit_rate_top1p[0],\n",
    "            'hr5p': hit_rate_top5p[0],\n",
    "            'hr10p': hit_rate_top10p[0],\n",
    "            'hr15p': hit_rate_top15p[0],\n",
    "            'ndcg1p': ndcg_score_1p,\n",
    "            'ndcg5p': ndcg_score_5p,\n",
    "            'ndcg10p': ndcg_score_10p,\n",
    "            'ndcg15p': ndcg_score_15p\n",
    "        };"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(10):\n",
    "    print(f\"Model {i+1}\")\n",
    "    res = train(regression_lr=LR_REGRESSION, bert_tweet_lr=LR_BERT_TWEET, bert_news_title_lr=LR_BERT_NEWS_TITLE, bert_news_text_lr=LR_BERT_NEWS_TEXT,\n",
    "          max_epoch=EPOCH, batch_size=BATCH_SIZE, early_stop_patience=EARLY_STOP_PATIENCE, verbose=True, manual_seed=None)\n",
    "    res['score'] = res['hr1p'] + res['hr5p'] + res['hr10p'] + res['hr15p'] + res['ndcg1p'] + res['ndcg5p'] + res['ndcg10p'] + res['ndcg15p']\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_results = sorted(results, key = lambda k: k['score'], reverse=True)\n",
    "sorted_results[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"0.0018, 0.0018, 0.0043, 0.0038, 0.0044, 0.0026, 0.0018, 0.0031\"\n",
    "s.replace(\",\", \"+\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
